{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301eefa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dopar\\anaconda3\\envs\\pyhome\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "from google import genai\n",
    "\n",
    "# PubMedQA 평가를 위한 라이브러리 추가\n",
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "client = genai.Client(api_key='AIzaSyClUpDU2mSLVLMnQvcJBLTiKFGjeComQTA')\n",
    "\n",
    "\n",
    "for m in client.models.list():\n",
    "  if 'embedContent' in m.supported_actions:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9f7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "  def __call__(self, input: Documents) -> Embeddings:\n",
    "    EMBEDDING_MODEL_ID = \"models/text-embedding-004\"\n",
    "    title = \"Custom query\"\n",
    "    response = client.models.embed_content(\n",
    "        model=EMBEDDING_MODEL_ID,\n",
    "        contents=input,\n",
    "        config=types.EmbedContentConfig(\n",
    "          task_type=\"retrieval_document\",\n",
    "          title=title\n",
    "        )\n",
    "    )\n",
    "    # 모든 문서의 임베딩 벡터를 리스트로 반환\n",
    "    return [e.values for e in response.embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322eb3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dopar\\AppData\\Local\\Temp\\ipykernel_19104\\4163188603.py:27: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  embedding_function=GeminiEmbeddingFunction()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing collection 'pubmedqa_long_answers' to delete or error during deletion: Collection [pubmedqa_long_answers] does not exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dopar\\AppData\\Local\\Temp\\ipykernel_19104\\4163188603.py:56: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  embedding_function=GeminiEmbeddingFunction()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and populated 'pubmedqa_long_answers' with 1000 documents.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#chroma_client.delete_collection(\"my_collection\")\n",
    "def preprocess_metadata(metadata):\n",
    "    new_metadata = {}\n",
    "    for k, v in metadata.items():\n",
    "        if isinstance(v, list):\n",
    "            new_metadata[k] = \", \".join(map(str, v))  # 리스트를 문자열로 변환\n",
    "        else:\n",
    "            new_metadata[k] = v\n",
    "    return new_metadata\n",
    "def batch_add(collection, documents, metadatas, ids, batch_size=100):\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch_docs = documents[i:i+batch_size]\n",
    "        batch_metas = metadatas[i:i+batch_size]\n",
    "        batch_ids = ids[i:i+batch_size]\n",
    "        collection.add(\n",
    "            documents=batch_docs,\n",
    "            metadatas=batch_metas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "def create_chroma_db(json_data, name):\n",
    "    import chromadb\n",
    "\n",
    "    chroma_client = chromadb.Client()\n",
    "    db = chroma_client.create_collection(\n",
    "        name=name,\n",
    "        embedding_function=GeminiEmbeddingFunction()\n",
    "    )\n",
    "\n",
    "    # JSON 데이터에서 텍스트, 메타데이터, id 추출\n",
    "    documents = [item[\"text\"] for item in json_data]\n",
    "    metadatas = [preprocess_metadata(item[\"metadata\"]) for item in json_data]\n",
    "    ids = [str(i) for i in range(len(json_data))]\n",
    "    \n",
    "    batch_add(db, documents, metadatas, ids, batch_size=100)\n",
    "   \n",
    "    return db\n",
    "with open('disease_rag_with_metadata.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "db = create_chroma_db(data, \"my_collection\")\n",
    "\n",
    "# PubMedQA long_answer 임베딩을 위한 코드 추가\n",
    "def create_pubmedqa_long_answer_embeddings(pubmedqa_dataset, collection_name=\"pubmedqa_long_answers\"):\n",
    "    chroma_client = chromadb.Client()\n",
    "    \n",
    "    # Check if collection already exists and delete it to start fresh\n",
    "    try:\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "        print(f\"Existing collection '{collection_name}' deleted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"No existing collection '{collection_name}' to delete or error during deletion: {e}\")\n",
    "\n",
    "    db_long_answers = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=GeminiEmbeddingFunction()\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    for i, entry in enumerate(pubmedqa_dataset['train']):\n",
    "        question = entry['question']\n",
    "        context = \" \".join(entry['context']['contexts'])\n",
    "        long_answer = entry['long_answer']\n",
    "        final_decision = entry['final_decision']\n",
    "\n",
    "        # Combine relevant information into a single document string for embedding\n",
    "        document_text = f\"Question: {question} Context: {context} Answer: {long_answer}\"\n",
    "        \n",
    "        documents.append(document_text)\n",
    "        metadatas.append({\n",
    "            \"question\": question,\n",
    "            \"final_decision\": final_decision,\n",
    "            \"context\": context\n",
    "        })\n",
    "        ids.append(str(i))\n",
    "    \n",
    "    batch_add(db_long_answers, documents, metadatas, ids, batch_size=100)\n",
    "    print(f\"Successfully created and populated '{collection_name}' with {len(documents)} documents.\")\n",
    "    return db_long_answers\n",
    "\n",
    "# PubMedQA 데이터셋 로드\n",
    "pubmedqa_dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "\n",
    "# long_answer 임베딩 및 Chroma DB 생성\n",
    "db_pubmedqa_long_answers = create_pubmedqa_long_answer_embeddings(pubmedqa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9cb907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  IDs                                          Documents  \\\n",
      "0   0  증상: Fever, Fatigue, Difficulty Breathing이(가) 있...   \n",
      "1   1  증상: Cough, Fatigue이(가) 있고, 나이: 25세, 성별: Female...   \n",
      "2   2  증상: Cough, Fatigue이(가) 있고, 나이: 25세, 성별: Female...   \n",
      "\n",
      "                                          Embeddings  \n",
      "0  [ 0.00477051  0.04616465 -0.06666899 -0.040444...  \n",
      "1  [-3.57129462e-02  7.49278739e-02 -6.56882823e-...  \n",
      "2  [-3.55055146e-02  4.89426367e-02 -6.10607862e-...  \n",
      "\n",
      "--- Sample PubMedQA Long Answer Embeddings ---\n",
      "  IDs                                          Documents  \\\n",
      "0   0  Question: Do mitochondria play a role in remod...   \n",
      "1   1  Question: Landolt C and snellen e acuity: diff...   \n",
      "2   2  Question: Syncope during bathing in infants, a...   \n",
      "\n",
      "                                           Metadatas  \\\n",
      "0  {'context': 'Programmed cell death (PCD) is th...   \n",
      "1  {'question': 'Landolt C and snellen e acuity: ...   \n",
      "2  {'final_decision': 'yes', 'question': 'Syncope...   \n",
      "\n",
      "                                          Embeddings  \n",
      "0  [-4.83142845e-02  3.91270705e-02 -6.88774511e-...  \n",
      "1  [ 4.58505452e-02  4.41183709e-03 -1.36079304e-...  \n",
      "2  [ 3.41068069e-03 -1.07322438e-02 -6.37394786e-...  \n"
     ]
    }
   ],
   "source": [
    "sample_data = db.get(include=['documents', 'embeddings'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"IDs\": sample_data['ids'][:3],\n",
    "    \"Documents\": sample_data['documents'][:3],\n",
    "    \"Embeddings\": [str(emb)[:50] + \"...\" for emb in sample_data['embeddings'][:3]]  # Truncate embeddings\n",
    "})\n",
    "print(df)\n",
    "\n",
    "sample_long_answer_data = db_pubmedqa_long_answers.get(include=['documents', 'embeddings', 'metadatas'], limit=3)\n",
    "df_long_answers = pd.DataFrame({\n",
    "    \"IDs\": sample_long_answer_data['ids'],\n",
    "    \"Documents\": sample_long_answer_data['documents'],\n",
    "    \"Metadatas\": sample_long_answer_data['metadatas'],\n",
    "    \"Embeddings\": [str(emb)[:50] + \"...\" for emb in sample_long_answer_data['embeddings']]\n",
    "})\n",
    "print(\"\\n--- Sample PubMedQA Long Answer Embeddings ---\")\n",
    "print(df_long_answers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4861eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using general disease database (profile-specific query detected).\n",
      "\n",
      "--- Passages for General Query ---\n",
      "증상: Fever이(가) 있고, 나이: 31세, 성별: Female, 혈압: Normal, 콜레스테롤: Normal인 환자의 경우 Migraine일 수 있습니다. (나이: 31, 성별: Female, 혈압: Normal, 콜레스테롤: Normal)\n",
      "증상: Fever이(가) 있고, 나이: 25세, 성별: Female, 혈압: Normal, 콜레스테롤: Normal인 환자의 경우 Eczema일 수 있습니다. (나이: 25, 성별: Female, 혈압: Normal, 콜레스테롤: Normal)\n",
      "증상: Fever, Cough, Difficulty Breathing이(가) 있고, 나이: 30세, 성별: Female, 혈압: Normal, 콜레스테롤: Normal인 환자의 경우 Asthma일 수 있습니다. (나이: 30, 성별: Female, 혈압: Normal, 콜레스테롤: Normal)\n",
      "증상: Fever, Cough, Difficulty Breathing이(가) 있고, 나이: 30세, 성별: Female, 혈압: Normal, 콜레스테롤: Normal인 환자의 경우 Asthma일 수 있습니다. (나이: 30, 성별: Female, 혈압: Normal, 콜레스테롤: Normal)\n",
      "증상: Fever, Difficulty Breathing이(가) 있고, 나이: 42세, 성별: Female, 혈압: Normal, 콜레스테롤: Normal인 환자의 경우 Liver Disease일 수 있습니다. (나이: 42, 성별: Female, 혈압: Normal, 콜레스테롤: Normal)\n",
      "--------------------------------------------------\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "\n",
      "--- Passages for Medical QA Query ---\n",
      "Question: Can tailored interventions increase mammography use among HMO women? Context: Telephone counseling and tailored print communications have emerged as promising methods for promoting mammography screening. However, there has been little research testing, within the same randomized field trial, of the efficacy of these two methods compared to a high-quality usual care system for enhancing screening. This study addressed the question: Compared to usual care, is tailored telephone counseling more effective than tailored print materials for promoting mammography screening? Three-year randomized field trial. One thousand ninety-nine women aged 50 and older recruited from a health maintenance organization in North Carolina. Women were randomized to 1 of 3 groups: (1) usual care, (2) tailored print communications, and (3) tailored telephone counseling. Adherence to mammography screening based on self-reports obtained during 1995, 1996, and 1997. Compared to usual care alone, telephone counseling promoted a significantly higher proportion of women having mammograms on schedule (71% vs 61%) than did tailored print (67% vs 61%) but only after the first year of intervention (during 1996). Furthermore, compared to usual care, telephone counseling was more effective than tailored print materials at promoting being on schedule with screening during 1996 and 1997 among women who were off-schedule during the previous year. Answer: Question: Can tailored interventions increase mammography use among HMO women? Context: Telephone counseling and tailored print communications have emerged as promising methods for promoting mammography screening. However, there has been little research testing, within the same randomized field trial, of the efficacy of these two methods compared to a high-quality usual care system for enhancing screening. This study addressed the question: Compared to usual care, is tailored telephone counseling more effective than tailored print materials for promoting mammography screening? Three-year randomized field trial. One thousand ninety-nine women aged 50 and older recruited from a health maintenance organization in North Carolina. Women were randomized to 1 of 3 groups: (1) usual care, (2) tailored print communications, and (3) tailored telephone counseling. Adherence to mammography screening based on self-reports obtained during 1995, 1996, and 1997. Compared to usual care alone, telephone counseling promoted a significantly higher proportion of women having mammograms on schedule (71% vs 61%) than did tailored print (67% vs 61%) but only after the first year of intervention (during 1996). Furthermore, compared to usual care, telephone counseling was more effective than tailored print materials at promoting being on schedule with screening during 1996 and 1997 among women who were off-schedule during the previous year. Answer: The effects of the intervention were most pronounced after the first intervention. Compared to usual care, telephone counseling seemed particularly effective at promoting change among nonadherent women, the group for whom the intervention was developed. These results suggest that telephone counseling, rather than tailored print, might be the preferred first-line intervention for getting nonadherent women on schedule for mammography screening. Many questions would have to be answered about why the tailored print intervention was not more powerful. Nevertheless, it is clear that additional interventions will be needed to maintain women's adherence to mammography. Medical Subject Headings (MeSH): mammography screening, telephone counseling, tailored print communications, barriers.\n",
      "Question: Prompting Primary Care Providers about Increased Patient Risk As a Result of Family History: Does It Work? Context: Electronic health records have the potential to facilitate family history use by primary care physicians (PCPs) to provide personalized care. The objective of this study was to determine whether automated, at-the-visit tailored prompts about family history risk change PCP behavior. Automated, tailored prompts highlighting familial risk for heart disease, stroke, diabetes, and breast, colorectal, or ovarian cancer were implemented during 2011 to 2012. Medical records of a cohort of community-based primary care patients, aged 35 to 65 years, who previously participated in our Family Healthware study and had a moderate or strong familial risk for any of the 6 diseases were subsequently reviewed. The main outcome measures were PCP response to the prompts, adding family history risk to problem summary lists, and patient screening status for each disease. The 492 eligible patients had 847 visits during the study period; 152 visits had no documentation of response to a family history prompt. Of the remaining 695 visits, physician responses were reviewed family history (n = 372, 53.5%), discussed family history (n = 159, 22.9%), not addressed (n = 155, 22.3%), and reviewed family history and ordered tests/referrals (n = 5, 0.7%). There was no significant change in problem summary list documentation of risk status or screening interventions for any of the 6 diseases. Answer: Question: Prompting Primary Care Providers about Increased Patient Risk As a Result of Family History: Does It Work? Context: Electronic health records have the potential to facilitate family history use by primary care physicians (PCPs) to provide personalized care. The objective of this study was to determine whether automated, at-the-visit tailored prompts about family history risk change PCP behavior. Automated, tailored prompts highlighting familial risk for heart disease, stroke, diabetes, and breast, colorectal, or ovarian cancer were implemented during 2011 to 2012. Medical records of a cohort of community-based primary care patients, aged 35 to 65 years, who previously participated in our Family Healthware study and had a moderate or strong familial risk for any of the 6 diseases were subsequently reviewed. The main outcome measures were PCP response to the prompts, adding family history risk to problem summary lists, and patient screening status for each disease. The 492 eligible patients had 847 visits during the study period; 152 visits had no documentation of response to a family history prompt. Of the remaining 695 visits, physician responses were reviewed family history (n = 372, 53.5%), discussed family history (n = 159, 22.9%), not addressed (n = 155, 22.3%), and reviewed family history and ordered tests/referrals (n = 5, 0.7%). There was no significant change in problem summary list documentation of risk status or screening interventions for any of the 6 diseases. Answer: No change occurred upon instituting simple, at-the-visit family history prompts geared to improve PCPs' ability to identify patients at high risk for 6 common conditions. The results are both surprising and disappointing. Further studies should examine physicians' perception of the utility of prompts for family history risk.\n",
      "Question: Mammographic screening in Sami speaking municipalities and a control group. Are early outcome measures influenced by ethnicity? Context: Female citizens of Sami (the indigenous people of Norway) municipalities in northern Norway have a low risk of breast cancer. The objective of this study was to describe the attendance rate and outcome of the Norwegian Breast Cancer Screening Program (NBCSP) in the Sami-speaking municipalities and a control group. A retrospective registry-based study. The 8 municipalities included in the administration area of the Sami language law (Sami) were matched with a control group of 11 municipalities (non-Sami). Population data were accessed from Statistics Norway. Data regarding invitations and outcome in the NBCSP during the period 2001-2010 was derived from the Cancer Registry of Norway (CRN). The NBCSP targets women aged 50-69 years. Rates and percentages were compared using chi-square test with a p-value<0.05 as statistical significant. The attendance rate in the NBCSP was 78% in the Sami and 75% in the non-Sami population (p<0.01). The recall rates were 2.4 and 3.3% in the Sami and non-Sami population, respectively (p<0.01). The rate of invasive screen detected cancer was not significantly lower in the Sami group (p=0.14). The percentage of all breast cancers detected in the NBCSP among the Sami (67%) was lower compared with the non-Sami population (86%, p=0.06). Answer: Question: Mammographic screening in Sami speaking municipalities and a control group. Are early outcome measures influenced by ethnicity? Context: Female citizens of Sami (the indigenous people of Norway) municipalities in northern Norway have a low risk of breast cancer. The objective of this study was to describe the attendance rate and outcome of the Norwegian Breast Cancer Screening Program (NBCSP) in the Sami-speaking municipalities and a control group. A retrospective registry-based study. The 8 municipalities included in the administration area of the Sami language law (Sami) were matched with a control group of 11 municipalities (non-Sami). Population data were accessed from Statistics Norway. Data regarding invitations and outcome in the NBCSP during the period 2001-2010 was derived from the Cancer Registry of Norway (CRN). The NBCSP targets women aged 50-69 years. Rates and percentages were compared using chi-square test with a p-value<0.05 as statistical significant. The attendance rate in the NBCSP was 78% in the Sami and 75% in the non-Sami population (p<0.01). The recall rates were 2.4 and 3.3% in the Sami and non-Sami population, respectively (p<0.01). The rate of invasive screen detected cancer was not significantly lower in the Sami group (p=0.14). The percentage of all breast cancers detected in the NBCSP among the Sami (67%) was lower compared with the non-Sami population (86%, p=0.06). Answer: Despite a lower risk of breast cancer, the Sami attended the NBCSP more frequently than the control group. The recall and cancer detection rate was lower among the Sami compared with the non-Sami group.\n",
      "Question: Does increased patient awareness improve accrual into cancer-related clinical trials? Context: Oncology literature cites that only 2% to 4% of patients participate in research. Up to 85% of patients are unaware that clinical trials research is being conducted at their treatment facility or that they might be eligible to participate. It was hypothesized that patients' satisfaction with information regarding clinical trials would improve after targeted educational interventions, and accruals to clinical trials would increase in the year following those interventions. All new patients referred to the cancer center over a 4-month period were mailed a baseline survey to assess their knowledge of clinical research. Subsequently, educational interventions were provided, including an orientation session highlighting clinical trials, a pamphlet, and a reference to a clinical trials Web site. A postintervention survey was sent to the responders of the initial survey 3 months after the initial mailing. Patient satisfaction with information significantly increased after the interventions. There was no increase in subsequent enrollment in clinical trials. Patients who indicated an inclination to participate in clinical trials tended to have greater satisfaction with the information they received. Answer: Question: Does increased patient awareness improve accrual into cancer-related clinical trials? Context: Oncology literature cites that only 2% to 4% of patients participate in research. Up to 85% of patients are unaware that clinical trials research is being conducted at their treatment facility or that they might be eligible to participate. It was hypothesized that patients' satisfaction with information regarding clinical trials would improve after targeted educational interventions, and accruals to clinical trials would increase in the year following those interventions. All new patients referred to the cancer center over a 4-month period were mailed a baseline survey to assess their knowledge of clinical research. Subsequently, educational interventions were provided, including an orientation session highlighting clinical trials, a pamphlet, and a reference to a clinical trials Web site. A postintervention survey was sent to the responders of the initial survey 3 months after the initial mailing. Patient satisfaction with information significantly increased after the interventions. There was no increase in subsequent enrollment in clinical trials. Patients who indicated an inclination to participate in clinical trials tended to have greater satisfaction with the information they received. Answer: A set of educational interventions designed for cancer patients significantly improved their satisfaction with information on clinical research, but did not improve clinical trial enrollment of these participants as of 1 year after the study.\n",
      "Question: The inverse equity hypothesis: does it apply to coverage of cancer screening in middle-income countries? Context: It is uncertain whether the inverse equity hypothesis-the idea that new health interventions are initially primarily accessed by the rich, but that inequalities narrow with diffusion to the poor-holds true for cancer screening in low and middle income countries (LMICs).This study examines the relationship between overall coverage and economic inequalities in coverage of cancer screening in four middle-income countries. Secondary analyses of cross-sectional data from the WHO study on Global Ageing and Adult Health in China, Mexico, Russia and South Africa (2007-2010). Three regression-based methods were used to measure economic inequalities: (1) Adjusted OR; (2) Relative Index of Inequality (RII); and (3) Slope Index of Inequality. Coverage for breast cancer screening was 10.5% in South Africa, 19.3% in China, 33.8% in Russia and 43% in Mexico, and coverage for cervical cancer screening was 24% in South Africa, 27.2% in China, 63.7% in Mexico and 81.5% in Russia. Economic inequalities in screening participation were substantially lower or non-existent in countries with higher aggregate coverage, for both breast cancer screening (RII: 14.57 in South Africa, 4.90 in China, 2.01 in Mexico, 1.04 in Russia) and cervical cancer screening (RII: 3.60 in China, 2.47 in South Africa, 1.39 in Mexico, 1.12 in Russia). Answer: Question: The inverse equity hypothesis: does it apply to coverage of cancer screening in middle-income countries? Context: It is uncertain whether the inverse equity hypothesis-the idea that new health interventions are initially primarily accessed by the rich, but that inequalities narrow with diffusion to the poor-holds true for cancer screening in low and middle income countries (LMICs).This study examines the relationship between overall coverage and economic inequalities in coverage of cancer screening in four middle-income countries. Secondary analyses of cross-sectional data from the WHO study on Global Ageing and Adult Health in China, Mexico, Russia and South Africa (2007-2010). Three regression-based methods were used to measure economic inequalities: (1) Adjusted OR; (2) Relative Index of Inequality (RII); and (3) Slope Index of Inequality. Coverage for breast cancer screening was 10.5% in South Africa, 19.3% in China, 33.8% in Russia and 43% in Mexico, and coverage for cervical cancer screening was 24% in South Africa, 27.2% in China, 63.7% in Mexico and 81.5% in Russia. Economic inequalities in screening participation were substantially lower or non-existent in countries with higher aggregate coverage, for both breast cancer screening (RII: 14.57 in South Africa, 4.90 in China, 2.01 in Mexico, 1.04 in Russia) and cervical cancer screening (RII: 3.60 in China, 2.47 in South Africa, 1.39 in Mexico, 1.12 in Russia). Answer: Economic inequalities in breast and cervical cancer screening are low in LMICs with high screening coverage. These findings are consistent with the inverse equity hypothesis and indicate that high levels of equity in cancer screening are feasible even in countries with high income inequality.\n",
      "--------------------------------------------------\n",
      "Using general disease database (similarity-based selection or default).\n",
      "\n",
      "--- Passages for Symptom Query ---\n",
      "증상: Fever, Fatigue, Difficulty Breathing이(가) 있고, 나이: 40세, 성별: Male, 혈압: Normal, 콜레스테롤: High인 환자의 경우 Asthma일 수 있습니다. (나이: 40, 성별: Male, 혈압: Normal, 콜레스테롤: High)\n",
      "증상: Fever, Fatigue, Difficulty Breathing이(가) 있고, 나이: 40세, 성별: Male, 혈압: Normal, 콜레스테롤: High인 환자의 경우 Asthma일 수 있습니다. (나이: 40, 성별: Male, 혈압: Normal, 콜레스테롤: High)\n",
      "증상: Fever, Fatigue, Difficulty Breathing이(가) 있고, 나이: 32세, 성별: Female, 혈압: High, 콜레스테롤: Normal인 환자의 경우 Pneumonia일 수 있습니다. (나이: 32, 성별: Female, 혈압: High, 콜레스테롤: Normal)\n",
      "증상: Fever, Cough, Fatigue, Difficulty Breathing이(가) 있고, 나이: 50세, 성별: Female, 혈압: Normal, 콜레스테롤: High인 환자의 경우 Asthma일 수 있습니다. (나이: 50, 성별: Female, 혈압: Normal, 콜레스테롤: High)\n",
      "증상: Fever, Cough, Fatigue, Difficulty Breathing이(가) 있고, 나이: 60세, 성별: Female, 혈압: High, 콜레스테롤: High인 환자의 경우 Asthma일 수 있습니다. (나이: 60, 성별: Female, 혈압: High, 콜레스테롤: High)\n"
     ]
    }
   ],
   "source": [
    "def get_relevant_passage(query, db, n_results=5):\n",
    "  results = db.query(query_texts=[query], n_results=n_results, include=['documents', 'metadatas'])\n",
    "  passages = []\n",
    "  for i in range(len(results['documents'][0])):\n",
    "      doc = results['documents'][0][i]\n",
    "      meta = results['metadatas'][0][i]\n",
    "      \n",
    "      passage_text = f\"{doc} (나이: {meta.get('age', '정보 없음')}, 성별: {meta.get('gender', '정보 없음')}, 혈압: {meta.get('blood_pressure', '정보 없음')}, 콜레스테롤: {meta.get('cholesterol', '정보 없음')})\"\n",
    "      passages.append(passage_text)\n",
    "  return passages\n",
    "# Perform embedding search\n",
    "passages = get_relevant_passage(\"Fever, Cough, Difficulty Breathing\", db, 5)\n",
    "\n",
    "\n",
    "def get_relevant_passage_intelligent(query, db_general, db_medical_qa, n_results=5):\n",
    "    medical_keywords = [\"medication\", \"treatment\", \"diagnosis\", \"clinical\", \"study\", \"trial\", \"gene\", \"protein\", \"cell\", \"molecule\", \"pubmed\", \"article\", \"research\", \"efficacy\", \"mechanism\", \"pathway\", \"therapy\", \"syndrome\", \"disorder\"]\n",
    "    \n",
    "    is_medical_query_keyword_detected = any(keyword in query.lower() for keyword in medical_keywords)\n",
    "    is_profile_specific_query_keyword_detected = bool(re.search(r'(나이|성별|혈압|콜레스테롤|증상):', query))\n",
    "\n",
    "    # 각 DB에서 잠재적 결과와 유사도 점수 가져오기\n",
    "    # include=['documents', 'metadatas', 'distances']를 추가하여 유사도 점수를 가져옵니다.\n",
    "    results_general = db_general.query(query_texts=[query], n_results=n_results, include=['documents', 'metadatas', 'distances'])\n",
    "    results_medical_qa = db_medical_qa.query(query_texts=[query], n_results=n_results, include=['documents', 'metadatas', 'distances'])\n",
    "\n",
    "    # 유사도 점수를 기반으로 더 적합한 DB 선택\n",
    "    # 유사도 점수는 낮을수록 더 가깝습니다 (거리 측정).\n",
    "    # 따라서 평균 거리가 더 낮은 DB를 선호합니다.\n",
    "    avg_distance_general = np.mean(results_general['distances'][0]) if results_general['distances'][0] else float('inf')\n",
    "    avg_distance_medical_qa = np.mean(results_medical_qa['distances'][0]) if results_medical_qa['distances'][0] else float('inf')\n",
    "\n",
    "    selected_db_name = \"\"\n",
    "    selected_results = None\n",
    "\n",
    "    if is_profile_specific_query_keyword_detected:\n",
    "        # 프로필 관련 키워드가 있으면 일반 질병 DB 우선 (현재 로직 유지)\n",
    "        print(\"Using general disease database (profile-specific query detected).\")\n",
    "        selected_db_name = \"general\"\n",
    "        selected_results = results_general\n",
    "    elif is_medical_query_keyword_detected:\n",
    "        # 의료 키워드가 있으면 PubMedQA DB 우선 (현재 로직 유지)\n",
    "        print(\"Using PubMedQA medical QA database (medical keywords detected).\")\n",
    "        selected_db_name = \"medical_qa\"\n",
    "        selected_results = results_medical_qa\n",
    "    else:\n",
    "        # 키워드 매칭이 없으면 유사도 점수를 기반으로 선택\n",
    "        if avg_distance_medical_qa < avg_distance_general:\n",
    "            print(\"Using PubMedQA medical QA database (similarity-based selection).\")\n",
    "            selected_db_name = \"medical_qa\"\n",
    "            selected_results = results_medical_qa\n",
    "        else:\n",
    "            print(\"Using general disease database (similarity-based selection or default).\")\n",
    "            selected_db_name = \"general\"\n",
    "            selected_results = results_general\n",
    "\n",
    "    passages = []\n",
    "    if selected_results:\n",
    "        for i in range(len(selected_results['documents'][0])):\n",
    "            doc = selected_results['documents'][0][i]\n",
    "            meta = selected_results['metadatas'][0][i]\n",
    "            \n",
    "            if selected_db_name == \"general\":\n",
    "                passage_text = f\"{doc} (나이: {meta.get('age', '정보 없음')}, 성별: {meta.get('gender', '정보 없음')}, 혈압: {meta.get('blood_pressure', '정보 없음')}, 콜레스테롤: {meta.get('cholesterol', '정보 없음')})\"\n",
    "            elif selected_db_name == \"medical_qa\":\n",
    "                passage_text = f\"Question: {meta.get('question', '정보 없음')} Context: {meta.get('context', '정보 없음')} Answer: {doc}\"\n",
    "            passages.append(passage_text)\n",
    "    else:\n",
    "        print(\"No suitable database found for the query.\")\n",
    "\n",
    "    return passages\n",
    "\n",
    "# 예시 사용:\n",
    "# 먼저, 위에서 정의한 db와 db_pubmedqa_long_answers가 생성되어 있어야 합니다.\n",
    "\n",
    "# 일반 질병 쿼리 예시\n",
    "query_general = \"나이: 30세, 성별: Female, 증상: 기침, 콧물, 인후통\"\n",
    "passages_general = get_relevant_passage_intelligent(query_general, db, db_pubmedqa_long_answers, 5)\n",
    "print(\"\\n--- Passages for General Query ---\")\n",
    "for p in passages_general:\n",
    "    print(p)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 의학 논문 쿼리 예시\n",
    "query_medical = \"Can tailored interventions increase mammography use among HMO women? A clinical study\"\n",
    "passages_medical = get_relevant_passage_intelligent(query_medical, db, db_pubmedqa_long_answers, 5)\n",
    "print(\"\\n--- Passages for Medical QA Query ---\")\n",
    "for p in passages_medical:\n",
    "    print(p)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 일반적인 증상 쿼리 (기본적으로 일반 질병 DB 사용)\n",
    "query_symptom = \"Fever, Fatigue, Difficulty Breathing\"\n",
    "passages_symptom = get_relevant_passage_intelligent(query_symptom, db, db_pubmedqa_long_answers, 5)\n",
    "print(\"\\n--- Passages for Symptom Query ---\")\n",
    "for p in passages_symptom:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c7eede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using general disease database (profile-specific query detected).\n",
      "--- LLM 답변 ---\n",
      "안녕하세요! 심각한 공부하기 싫음 증상으로 힘드시군요. 학업에 대한 의욕이 저하되는 것은 누구에게나 있을 수 있는 일이지만, 그 원인을 파악하고 적절히 대처하는 것이 중요합니다.\n",
      "\n",
      "현재 제공된 임상 데이터에서는 '심각한 공부하기 싫음'과 직접적으로 연결된 질병 정보는 찾을 수 없었습니다. 하지만, 25세 남성분의 경우, 학업 스트레스나 다른 요인으로 인해 발생할 수 있는 일반적인 상황들을 고려하여 몇 가지 조언을 드릴 수 있습니다.\n",
      "\n",
      "'심각한 공부하기 싫음'은 단순히 의지의 문제가 아니라, 스트레스, 수면 부족, 영양 불균형, 환경적인 요인 등 다양한 원인에 의해 발생할 수 있습니다. 일시적인 무기력감일 수도 있지만, 장기적으로 지속될 경우 학업 성취도 저하, 우울감, 불안감 등 다른 문제로 이어질 수도 있습니다.\n",
      "\n",
      "이럴 때는 다음과 같은 생활 습관 개선을 통해 증상 완화에 도움을 줄 수 있습니다:\n",
      "\n",
      "- **충분한 휴식:** 규칙적인 수면 습관을 유지하고, 낮 시간 동안 짧은 낮잠이나 휴식을 취하는 것이 좋습니다.\n",
      "- **균형 잡힌 식단:** 인스턴트 음식이나 가공 식품보다는 신선한 과일, 채소, 단백질 위주의 식단을 섭취하여 뇌 기능 활성화에 도움을 주세요.\n",
      "- **적절한 운동:** 가벼운 산책이나 스트레칭, 요가 등을 통해 신체 활동량을 늘리면 스트레스 해소와 집중력 향상에 도움이 됩니다.\n",
      "- **취미 활동:** 공부 외에 좋아하는 활동을 통해 스트레스를 해소하고 긍정적인 에너지를 얻으세요.\n",
      "- **학습 환경 개선:** 조용하고 쾌적한 환경에서 공부하고, 집중력을 방해하는 요소들을 최소화하세요.\n",
      "- **학습 방법 변화:** 단순히 암기하는 것보다 흥미를 느낄 수 있는 학습 방법을 찾거나, 스터디 그룹에 참여하여 동기 부여를 받는 것도 좋은 방법입니다.\n",
      "\n",
      "만약 '심각한 공부하기 싫음' 증상이 2주 이상 지속되거나, 다른 신체적, 정신적인 불편함이 동반된다면 전문가의 도움을 받는 것을 고려해 보세요. (드물게는, 만성피로증후군이나 우울증과 같은 질환의 초기 증상일 수도 있습니다.)\n",
      "\n",
      "더 궁금한 점이 있으시면 언제든지 다시 질문해주세요. 항상 건강하고 즐거운 학업 생활을 응원합니다!\n"
     ]
    }
   ],
   "source": [
    "def make_prompt(query, relevant_passages):\n",
    "  escaped = \" \".join([p.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \") for p in relevant_passages])\n",
    "  prompt = f\"\"\"\n",
    "  당신은 사용자의 증상과 개인 프로필 정보를 기반으로 질병을 설명하고, **일상생활에서 할 수 있는 구체적이고 실용적인 조언을 상세하게 제공하는** 의료 상담 도우미입니다. 당신의 답변은 정보가 풍부하고 친절하며, **극단적이거나 심각한 질병을 직접적으로 진단하거나 추천하는 뉘앙스를 피해야 합니다.** 답변은 최소 100단어 이상으로 작성해 주세요.\n",
    "\n",
    "  **단계별 지시사항:**\n",
    "  1. 사용자 질문을 이해하고 핵심 증상(예: 기침, 콧물)을 정확하고 상세하게 파악하세요.\n",
    "  2. 제공된 '관련 정보 (PASSAGE)'를 면밀히 검토하여 사용자 증상과 가장 밀접하게 일치하는 질병(들)을 식별하되, **데이터에 기반한 질병 연관성을 언급하되 불필요하게 심각성을 강조하지 마세요.**\n",
    "  3. **여러 질병이 검색될 경우, 가장 일반적이거나 흔한 질환(예: 감기, 알레르기)을 우선적으로 상세히 설명하고, 그 다음으로 관련된 다른 질병들도 간략하게 제시하세요.**\n",
    "  4. 나이, 성별, 혈압, 콜레스테롤 수치와 같은 환자 프로필 정보가 있다면, 이를 **답변의 서론 부분에 해당 질병이 특정 프로필의 환자에게서 관찰될 수 있는 '사례'로 자연스럽게 통합하여 설명의 깊이를 더하세요.** 질병 진단의 직접적인 근거로 오해되지 않도록 주의하세요.\n",
    "  5. 답변은 정보가 풍부하고 명확하며 친절하게 작성하며, 다음 **상세 권장 출력 형식**을 따르되, **세부적인 구문은 모델의 자연스러운 생성에 맡기세요.**\n",
    "\n",
    "  **상세 권장 출력 형식:**\n",
    "  안녕하세요! [사용자 질문에서 파악된 증상]이(가) 있으시군요. 불편하시겠지만, 몇 가지 가능한 원인과 생활 속 대처법을 함께 알아보겠습니다.\n",
    "\n",
    "  (선택적: 임상 데이터에 따르면, [관련 정보의 나이]세 [관련 정보의 성별] 환자 중 [관련 정보의 혈압] 혈압과 [관련 정보의 콜레스테롤] 콜레스테롤 수치를 가진 분들에게서 [해당 질병과 연결된 증상]이 관찰된 사례가 있습니다.) 이러한 증상들은 [관련 정보에서 찾은 가장 일반적이고 가능성 높은 질병]과 관련이 있을 수 있습니다.\n",
    "\n",
    "  [질병에 대한 간략한 추가 설명 (2-3문장)]. 이 질병의 일반적인 경과나 특징에 대해 간략히 설명해 주세요.\n",
    "\n",
    "  이럴 때는 다음과 같은 생활 습관 개선을 통해 증상 완화에 도움을 줄 수 있습니다:\n",
    "  - **충분한 휴식:** 몸이 회복하는 데 필요한 시간을 주세요.\n",
    "  - **수분 섭취:** 따뜻한 물, 차 등을 자주 마셔 목을 촉촉하게 유지하고 탈수를 예방하세요.\n",
    "  - **실내 환경 관리:** 적절한 실내 습도를 유지하고 환기를 자주 해주세요.\n",
    "  - **영양가 있는 음식 섭취:** 면역력 강화를 위해 비타민과 미네랄이 풍부한 음식을 드세요.\n",
    "  - [추가적인 일반적인 조언 1 (예: 스트레스 관리, 가벼운 운동 등)]\n",
    "  - [추가적인 일반적인 조언 2 (예: 마스크 착용, 손 씻기 등)]\n",
    "\n",
    "  만약 [사용자 질문에서 파악된 증상] 외에 다른 불편한 증상이 있거나, 현재 증상이 나아지지 않고 오히려 심해진다면 [다른 관련 질병]일 수도 있습니다. (이때, 극단적인 질병은 가급적 언급하지 않거나, \"드물게는 ~일 수도 있습니다\"와 같이 조심스러운 표현을 사용하세요.)\n",
    "\n",
    "  더 궁금한 점이 있으시면 언제든지 다시 질문해주세요. 항상 건강하시길 바랍니다.\n",
    "\n",
    "  아래는 참고할 수 있는 임상 데이터입니다:\n",
    "  - 사용자 질문 (QUESTION): \\\"{query}\\\"\n",
    "  - 관련 정보 (PASSAGE): \\\"{escaped}\\\"\n",
    "\n",
    "  **주의사항:**\n",
    "  - 병원 방문 및 전문적인 상담을 직접적으로 권유하는 문구는 최종 답변에 포함하지 마세요.\n",
    "  - PASSAGE에 영어 단어가 포함되어 있다면, 괄호 안에 한글 뜻을 함께 제공해 주세요.\n",
    "  - **제공된 정보 내에서 '기침'과 '습진'의 연관성이 있더라도, '기침'이라는 증상에 더 일반적이고 흔한 질병(예: Common Cold, Influenza)이 있다면 이를 우선적으로 고려하여 답변하세요.**\n",
    "  - **'말라리아'와 같이 심각한 질병은 사용자가 직접적으로 언급하지 않는 한, 일반적인 증상만으로는 추천하지 마세요.**\n",
    "\n",
    "  ANSWER:\n",
    "  \"\"\".format(query=query, relevant_passages=escaped)\n",
    "  return prompt\n",
    "# 예시 사용\n",
    "query = \"나이: 25세, 성별: Male, 혈압: Normal, 콜레스테롤: Normal, 증상 : 심각한 공부하기 싫음\"\n",
    "passages = get_relevant_passage_intelligent(query_general, db, db_pubmedqa_long_answers, 5)\n",
    "prompt = make_prompt(query, passages)\n",
    "\n",
    "#print(prompt)\n",
    "\n",
    "MODEL_ID = \"gemini-2.0-flash\"\n",
    "answer = client.models.generate_content(\n",
    "    model = MODEL_ID,\n",
    "    contents = prompt\n",
    ")\n",
    "# 변경 시작: 'ANSWER:' 이후의 텍스트만 추출하고 앞뒤 공백 제거\n",
    "final_answer = answer.text.split(\"ANSWER:\")\n",
    "if len(final_answer) > 1:\n",
    "    final_answer = final_answer[1].strip()\n",
    "else:\n",
    "    final_answer = answer.text.strip()\n",
    "   \n",
    "# 6. 결과 출력\n",
    "print(\"--- LLM 답변 ---\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d5df3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PubMedQA dataset...\n",
      "PubMedQA test set loaded with 1000 examples.\n",
      "\n",
      "Evaluating on 50 PubMedQA samples for Accuracy...\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 15)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 16)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 17)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 19)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 35)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 36)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 39)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 40)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 41)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 42)\n",
      "\n",
      "--- PubMedQA 평가 결과 (정확도) ---\n",
      "평가 샘플 수: 40\n",
      "정확도: 0.0750\n",
      "\n",
      "--- PubMedQA 평가 예시 (정확도) ---\n",
      "질문: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\n",
      "정답 (Ground Truth Decision): yes\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n",
      "질문: Landolt C and snellen e acuity: differences in strabismus amblyopia?\n",
      "정답 (Ground Truth Decision): no\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n",
      "질문: Syncope during bathing in infants, a pediatric form of water-induced urticaria?\n",
      "정답 (Ground Truth Decision): yes\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n",
      "질문: Are the long-term results of the transanal pull-through equal to those of the transabdominal pull-through?\n",
      "정답 (Ground Truth Decision): no\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n",
      "질문: Can tailored interventions increase mammography use among HMO women?\n",
      "정답 (Ground Truth Decision): yes\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 기존 make_prompt_for_pubmedqa 함수는 긴 답변을 유도하므로, 짧은 답변을 위한 새로운 프롬프트 함수가 필요합니다.\n",
    "def make_short_answer_prompt_for_pubmedqa(query, relevant_passages):\n",
    "    escaped_passages = \" \".join([p.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\\\n\", \" \") for p in relevant_passages])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    당신은 의학적 질문에 대해 'Yes', 'No', 'Maybe'로만 답변하는 의료 AI 도우미입니다.\n",
    "    제공된 '참고 정보'를 바탕으로 '의학적 질문'에 대한 답변을 생성하세요.\n",
    "\n",
    "    **다음 지침을 엄격히 따르세요:**\n",
    "    1.  **'Yes'로 답변하는 경우:** '참고 정보'에 '의학적 질문'에 대한 직접적이고 명확한 긍정적 증거가 있을 경우에 'Yes'라고 답변하세요.\n",
    "    2.  **'No'로 답변하는 경우:** '참고 정보'에 '의학적 질문'에 대한 직접적이고 명확한 부정적 증거가 있거나, 질문의 내용이 '참고 정보'와 명백히 상반될 경우에 'No'라고 답변하세요.\n",
    "    3.  **'No'로 답변하는 경우:** 만약 확실한 근거가 없다면 'Maybe'라고 답변하세요. 그러나 긍정적/부정적 근거가 명확하다면 반드시 'Yes' 또는 'No'로 답변해야 합니다\n",
    "    다른 어떠한 설명도 추가하지 말고, 오직 하나의 단어('Yes', 'No', 'Maybe')로만 답변해야 합니다.\n",
    "\n",
    "    **의학적 질문 (QUESTION):** \\\"{query}\\\"\n",
    "    **참고 정보 (PASSAGE):** \\\"{escaped_passages}\\\"\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 평가 함수 변경: ROUGE 대신 정확도 측정\n",
    "def evaluate_accuracy_pubmedqa(llm_response: str, ground_truth_decision: str) -> bool:\n",
    "    # 모델의 응답을 정규화 (대소문자 무시, 공백 제거 등)\n",
    "    normalized_llm_response = llm_response.strip().lower()\n",
    "    normalized_ground_truth = ground_truth_decision.strip().lower()\n",
    "\n",
    "    return normalized_llm_response == normalized_ground_truth\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 메인 평가 실행 부분 수정\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nLoading PubMedQA dataset...\")\n",
    "pubmedqa_dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "pubmedqa_test_data = pubmedqa_dataset['train']\n",
    "\n",
    "print(f\"PubMedQA test set loaded with {len(pubmedqa_test_data)} examples.\")\n",
    "\n",
    "all_accuracies = []\n",
    "example_evaluations = []\n",
    "\n",
    "num_samples_to_evaluate = 50\n",
    "\n",
    "print(f\"\\nEvaluating on {num_samples_to_evaluate} PubMedQA samples for Accuracy...\")\n",
    "\n",
    "for i, entry in enumerate(pubmedqa_test_data):\n",
    "    if i >= num_samples_to_evaluate:\n",
    "        break\n",
    "\n",
    "    question = entry['question']\n",
    "    ground_truth_decision = entry['final_decision'] # 'long_answer' 대신 'final_decision' 사용\n",
    "\n",
    "    relevant_passages = get_relevant_passage(question, db, 5)\n",
    "\n",
    "    # 새로운 짧은 답변 프롬프트 사용\n",
    "    prompt = make_short_answer_prompt_for_pubmedqa(question, relevant_passages)\n",
    "\n",
    "    try:\n",
    "        MODEL_ID = \"gemini-2.0-flash\"\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=prompt\n",
    "        )\n",
    "\n",
    "        llm_response = response.text.split(\"ANSWER:\")\n",
    "        if len(llm_response) > 1:\n",
    "            llm_response = llm_response[1].strip()\n",
    "        else:\n",
    "            llm_response = response.text.strip()\n",
    "\n",
    "        # 정확도 평가\n",
    "        is_correct = evaluate_accuracy_pubmedqa(llm_response, ground_truth_decision)\n",
    "        all_accuracies.append(is_correct)\n",
    "\n",
    "        if i < 5:\n",
    "            example_evaluations.append({\n",
    "                'question': question,\n",
    "                'ground_truth_decision': ground_truth_decision,\n",
    "                'llm_response': llm_response,\n",
    "                'is_correct': is_correct\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"429 RESOURCE_EXHAUSTED\" in str(e):\n",
    "            retry_after_seconds = 5\n",
    "            print(f\"Quota exceeded. Retrying after {retry_after_seconds} seconds... (Sample {i})\")\n",
    "            time.sleep(retry_after_seconds)\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Error processing sample {i}: {e}\")\n",
    "            all_accuracies.append(False) # 오류 발생 시 오답으로 처리\n",
    "\n",
    "# 평균 정확도 계산\n",
    "if all_accuracies:\n",
    "    total_correct = sum(all_accuracies)\n",
    "    accuracy = total_correct / len(all_accuracies)\n",
    "    print(\"\\n--- PubMedQA 평가 결과 (정확도) ---\")\n",
    "    print(f\"평가 샘플 수: {len(all_accuracies)}\")\n",
    "    print(f\"정확도: {accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo evaluation results to display.\")\n",
    "\n",
    "# 예시 평가 결과 출력\n",
    "print(\"\\n--- PubMedQA 평가 예시 (정확도) ---\")\n",
    "for ex in example_evaluations:\n",
    "    print(f\"질문: {ex['question']}\")\n",
    "    print(f\"정답 (Ground Truth Decision): {ex['ground_truth_decision']}\")\n",
    "    print(f\"LLM 응답: {ex['llm_response']}\")\n",
    "    print(f\"정확도 일치 여부: {ex['is_correct']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5206714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PubMedQA dataset...\n",
      "PubMedQA test set loaded with 1000 examples.\n",
      "\n",
      "Evaluating on 50 PubMedQA samples for Accuracy...\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 8)\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 11)\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 13)\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 30)\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 31)\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 32)\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 33)\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "Using PubMedQA medical QA database (similarity-based selection).\n",
      "\n",
      "--- PubMedQA 평가 결과 (정확도) ---\n",
      "평가 샘플 수: 43\n",
      "정확도: 0.7442\n",
      "\n",
      "--- PubMedQA 평가 예시 (정확도) ---\n",
      "질문: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\n",
      "정답 (Ground Truth Decision): yes\n",
      "LLM 응답: Yes\n",
      "정확도 일치 여부: True\n",
      "--------------------------------------------------\n",
      "질문: Landolt C and snellen e acuity: differences in strabismus amblyopia?\n",
      "정답 (Ground Truth Decision): no\n",
      "LLM 응답: Yes\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n",
      "질문: Syncope during bathing in infants, a pediatric form of water-induced urticaria?\n",
      "정답 (Ground Truth Decision): yes\n",
      "LLM 응답: Yes\n",
      "정확도 일치 여부: True\n",
      "--------------------------------------------------\n",
      "질문: Are the long-term results of the transanal pull-through equal to those of the transabdominal pull-through?\n",
      "정답 (Ground Truth Decision): no\n",
      "LLM 응답: No\n",
      "정확도 일치 여부: True\n",
      "--------------------------------------------------\n",
      "질문: Can tailored interventions increase mammography use among HMO women?\n",
      "정답 (Ground Truth Decision): yes\n",
      "LLM 응답: Yes\n",
      "정확도 일치 여부: True\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 기존 make_prompt_for_pubmedqa 함수는 긴 답변을 유도하므로, 짧은 답변을 위한 새로운 프롬프트 함수가 필요합니다.\n",
    "def make_short_answer_prompt_for_pubmedqa(query, relevant_passages):\n",
    "    escaped_passages = \" \".join([p.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\\\n\", \" \") for p in relevant_passages])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    당신은 의학적 질문에 대해 'Yes', 'No', 'Maybe'로만 답변하는 의료 AI 도우미입니다.\n",
    "    제공된 '참고 정보'를 바탕으로 '의학적 질문'에 대한 답변을 생성하세요.\n",
    "\n",
    "    **다음 지침을 엄격히 따르세요:**\n",
    "    1.  **'Yes'로 답변하는 경우:** '참고 정보'에 '의학적 질문'에 대한 직접적이고 명확한 긍정적 증거가 있을 경우에 'Yes'라고 답변하세요.\n",
    "    2.  **'No'로 답변하는 경우:** '참고 정보'에 '의학적 질문'에 대한 직접적이고 명확한 부정적 증거가 있거나, 질문의 내용이 '참고 정보'와 명백히 상반될 경우에 'No'라고 답변하세요.\n",
    "    3.  **'No'로 답변하는 경우:** 만약 확실한 근거가 없다면 'Maybe'라고 답변하세요. 그러나 긍정적/부정적 근거가 명확하다면 반드시 'Yes' 또는 'No'로 답변해야 합니다\n",
    "    다른 어떠한 설명도 추가하지 말고, 오직 하나의 단어('Yes', 'No', 'Maybe')로만 답변해야 합니다.\n",
    "\n",
    "    **의학적 질문 (QUESTION):** \\\"{query}\\\"\n",
    "    **참고 정보 (PASSAGE):** \\\"{escaped_passages}\\\"\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 평가 함수 변경: ROUGE 대신 정확도 측정\n",
    "def evaluate_accuracy_pubmedqa(llm_response: str, ground_truth_decision: str) -> bool:\n",
    "    # 모델의 응답을 정규화 (대소문자 무시, 공백 제거 등)\n",
    "    normalized_llm_response = llm_response.strip().lower()\n",
    "    normalized_ground_truth = ground_truth_decision.strip().lower()\n",
    "\n",
    "    return normalized_llm_response == normalized_ground_truth\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 메인 평가 실행 부분 수정\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nLoading PubMedQA dataset...\")\n",
    "pubmedqa_dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "pubmedqa_test_data = pubmedqa_dataset['train']\n",
    "\n",
    "print(f\"PubMedQA test set loaded with {len(pubmedqa_test_data)} examples.\")\n",
    "\n",
    "all_accuracies = []\n",
    "example_evaluations = []\n",
    "\n",
    "num_samples_to_evaluate = 50\n",
    "\n",
    "print(f\"\\nEvaluating on {num_samples_to_evaluate} PubMedQA samples for Accuracy...\")\n",
    "\n",
    "for i, entry in enumerate(pubmedqa_test_data):\n",
    "    if i >= num_samples_to_evaluate:\n",
    "        break\n",
    "\n",
    "    question = entry['question']\n",
    "    ground_truth_decision = entry['final_decision'] # 'long_answer' 대신 'final_decision' 사용\n",
    "\n",
    "    relevant_passages = get_relevant_passage_intelligent(question, db, db_pubmedqa_long_answers, 5)\n",
    "\n",
    "    # 새로운 짧은 답변 프롬프트 사용\n",
    "    prompt = make_short_answer_prompt_for_pubmedqa(question, relevant_passages)\n",
    "\n",
    "    try:\n",
    "        MODEL_ID = \"gemini-2.0-flash\"\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=prompt\n",
    "        )\n",
    "\n",
    "        llm_response = response.text.split(\"ANSWER:\")\n",
    "        if len(llm_response) > 1:\n",
    "            llm_response = llm_response[1].strip()\n",
    "        else:\n",
    "            llm_response = response.text.strip()\n",
    "\n",
    "        # 정확도 평가\n",
    "        is_correct = evaluate_accuracy_pubmedqa(llm_response, ground_truth_decision)\n",
    "        all_accuracies.append(is_correct)\n",
    "\n",
    "        if i < 5:\n",
    "            example_evaluations.append({\n",
    "                'question': question,\n",
    "                'ground_truth_decision': ground_truth_decision,\n",
    "                'llm_response': llm_response,\n",
    "                'is_correct': is_correct\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"429 RESOURCE_EXHAUSTED\" in str(e):\n",
    "            retry_after_seconds = 5\n",
    "            print(f\"Quota exceeded. Retrying after {retry_after_seconds} seconds... (Sample {i})\")\n",
    "            time.sleep(retry_after_seconds)\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Error processing sample {i}: {e}\")\n",
    "            all_accuracies.append(False) # 오류 발생 시 오답으로 처리\n",
    "\n",
    "# 평균 정확도 계산\n",
    "if all_accuracies:\n",
    "    total_correct = sum(all_accuracies)\n",
    "    accuracy = total_correct / len(all_accuracies)\n",
    "    print(\"\\n--- PubMedQA 평가 결과 (정확도) ---\")\n",
    "    print(f\"평가 샘플 수: {len(all_accuracies)}\")\n",
    "    print(f\"정확도: {accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo evaluation results to display.\")\n",
    "\n",
    "# 예시 평가 결과 출력\n",
    "print(\"\\n--- PubMedQA 평가 예시 (정확도) ---\")\n",
    "for ex in example_evaluations:\n",
    "    print(f\"질문: {ex['question']}\")\n",
    "    print(f\"정답 (Ground Truth Decision): {ex['ground_truth_decision']}\")\n",
    "    print(f\"LLM 응답: {ex['llm_response']}\")\n",
    "    print(f\"정확도 일치 여부: {ex['is_correct']}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
