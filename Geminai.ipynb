{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "301eefa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "from google import genai\n",
    "\n",
    "client = genai.Client(api_key='AIzaSyCHcy8oO-XhjsLCTdzLB64t9XR01OanbpM')\n",
    "\n",
    "\n",
    "for m in client.models.list():\n",
    "  if 'embedContent' in m.supported_actions:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab9f7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "  def __call__(self, input: Documents) -> Embeddings:\n",
    "    EMBEDDING_MODEL_ID = \"models/embedding-001\"\n",
    "    title = \"Custom query\"\n",
    "    response = client.models.embed_content(\n",
    "        model=EMBEDDING_MODEL_ID,\n",
    "        contents=input,\n",
    "        config=types.EmbedContentConfig(\n",
    "          task_type=\"retrieval_document\",\n",
    "          title=title\n",
    "        )\n",
    "    )\n",
    "    # 모든 문서의 임베딩 벡터를 리스트로 반환\n",
    "    return [e.values for e in response.embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "322eb3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\eh589\\AppData\\Local\\Temp\\ipykernel_10176\\75548902.py:27: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  embedding_function=GeminiEmbeddingFunction()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#chroma_client.delete_collection(\"my_collection\")\n",
    "def preprocess_metadata(metadata):\n",
    "    new_metadata = {}\n",
    "    for k, v in metadata.items():\n",
    "        if isinstance(v, list):\n",
    "            new_metadata[k] = \", \".join(map(str, v))  # 리스트를 문자열로 변환\n",
    "        else:\n",
    "            new_metadata[k] = v\n",
    "    return new_metadata\n",
    "def batch_add(collection, documents, metadatas, ids, batch_size=100):\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch_docs = documents[i:i+batch_size]\n",
    "        batch_metas = metadatas[i:i+batch_size]\n",
    "        batch_ids = ids[i:i+batch_size]\n",
    "        collection.add(\n",
    "            documents=batch_docs,\n",
    "            metadatas=batch_metas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "def create_chroma_db(json_data, name):\n",
    "    import chromadb\n",
    "\n",
    "    chroma_client = chromadb.Client()\n",
    "    db = chroma_client.create_collection(\n",
    "        name=name,\n",
    "        embedding_function=GeminiEmbeddingFunction()\n",
    "    )\n",
    "\n",
    "    # JSON 데이터에서 텍스트, 메타데이터, id 추출\n",
    "    documents = [item[\"text\"] for item in json_data]\n",
    "    metadatas = [preprocess_metadata(item[\"metadata\"]) for item in json_data]\n",
    "    ids = [str(i) for i in range(len(json_data))]\n",
    "    \n",
    "    batch_add(db, documents, metadatas, ids, batch_size=100)\n",
    "   \n",
    "    return db\n",
    "with open('disease_rag_with_metadata.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "db = create_chroma_db(data, \"my_collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9cb907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  IDs                                          Documents  \\\n",
      "0   0  Fever, Fatigue, Difficulty Breathing 증상이 있는 경우...   \n",
      "1   1      Cough, Fatigue 증상이 있는 경우 Common Cold일 수 있습니다.   \n",
      "2   2           Cough, Fatigue 증상이 있는 경우 Eczema일 수 있습니다.   \n",
      "\n",
      "                                          Embeddings  \n",
      "0  [ 3.16955782e-02 -4.14985903e-02 -4.31520194e-...  \n",
      "1  [ 7.59642050e-02 -6.42082170e-02 -7.00410604e-...  \n",
      "2  [ 6.10879771e-02 -5.23152687e-02 -7.92896152e-...  \n"
     ]
    }
   ],
   "source": [
    "sample_data = db.get(include=['documents', 'embeddings'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"IDs\": sample_data['ids'][:3],\n",
    "    \"Documents\": sample_data['documents'][:3],\n",
    "    \"Embeddings\": [str(emb)[:50] + \"...\" for emb in sample_data['embeddings'][:3]]  # Truncate embeddings\n",
    "})\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f4861eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fever, Fatigue, Difficulty Breathing 증상이 있는 경우 Asthma일 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "def get_relevant_passage(query, db):\n",
    "  passage = db.query(query_texts=[query], n_results=1)['documents'][0][0]\n",
    "  return passage\n",
    "# Perform embedding search\n",
    "passage = get_relevant_passage(\"Fever, Cough, Difficulty Breathing\", db)\n",
    "print(passage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c7eede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    당신은 의학 상담 도우미입니다.\n",
      "    다음은 참고할 증상 데이터입니다:\n",
      "    QUESTION: '나는 감기와 습진이 있어'\n",
      "    PASSAGE: 'Fever, Fatigue, Difficulty Breathing 증상이 있는 경우 Asthma일 수 있습니다.'\n",
      "    이 정보를 참고하여 사용자 질문에 답하세요:\n",
      "    ANSWER:\n",
      "  \n",
      "죄송하지만, 제공된 정보만으로는 사용자의 질문에 직접적으로 답변할 수 없습니다.\n",
      "\n",
      "*   사용자는 감기와 습진이 있다고 했습니다.\n",
      "*   제공된 정보는 발열, 피로, 호흡 곤란 증상이 천식일 수 있다는 내용입니다.\n",
      "\n",
      "이 두 정보는 직접적인 관련이 없어, 어떤 답변을 제공해야 할지 알 수 없습니다.\n",
      "\n",
      "더 정확한 답변을 위해서는 다음 중 하나의 정보가 필요합니다.\n",
      "\n",
      "*   감기나 습진과 관련된 추가적인 증상 정보\n",
      "*   천식 가능성에 대한 질문 여부\n",
      "*   원하는 답변의 종류 (예: 자가 치료 방법, 병원 방문 필요성 등)\n",
      "\n",
      "예를 들어, 사용자가 \"감기와 습진 때문에 기침이 심해요\" 라고 질문했다면, \"제공된 정보에 따르면 호흡 곤란 증상이 있는 경우 천식일 수 있습니다. 기침이 심하고 호흡이 불편하다면 병원을 방문하여 진료를 받아보는 것이 좋습니다.\" 와 같이 답변할 수 있습니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_relevant_passage(query, db):\n",
    "  passage = db.query(query_texts=[query], n_results=1)['documents'][0][0]\n",
    "  return passage\n",
    "# Perform embedding search\n",
    "passage = get_relevant_passage(\"Fever, Cough, Difficulty Breathing\", db)\n",
    "print(passage)\n",
    "def make_prompt(query, relevant_passage):\n",
    "  escaped = relevant_passage.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \")\n",
    "  prompt = (\"\"\"\n",
    "    당신은 의학 상담 도우미입니다.\n",
    "    다음은 참고할 증상 데이터입니다:\n",
    "    QUESTION: '{query}'\n",
    "    PASSAGE: '{relevant_passage}'\n",
    "    이 정보를 참고하여 사용자 질문에 답하세요:\n",
    "    ANSWER:\n",
    "  \"\"\").format(query=query, relevant_passage=escaped)\n",
    "  return prompt\n",
    "query = \"나는 감기와 습진이 있어\"\n",
    "prompt = make_prompt(query, passage)\n",
    "print(prompt)\n",
    "MODEL_ID = \"gemini-2.0-flash\"  # @param [\"gemini-2.0-flash-lite\", \"gemini-2.0-flash\", \"gemini-2.5-flash-preview-04-17\",\"gemini-2.5-pro-exp-05-06\"] {\"allow-input\": true, \"isTemplate\": true}\n",
    "answer = client.models.generate_content(\n",
    "    model = MODEL_ID,\n",
    "    contents = prompt\n",
    ")\n",
    "print(answer.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f1c1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chromadb import Client\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# ▶️ 임베딩 모델\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "class MyEmbeddingFunction:\n",
    "    def __init__(self, model_name=\"all-MiniLM-L6-v2\"):\n",
    "        from sentence_transformers import SentenceTransformer\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "\n",
    "    def __call__(self, input):\n",
    "        return self.model.encode(input).tolist()\n",
    "\n",
    "embedding_fn = MyEmbeddingFunction()\n",
    "\n",
    "# ▶️ Chroma client\n",
    "client = Client()\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"medical_rag\",\n",
    "    embedding_function=embedding_fn\n",
    ")\n",
    "\n",
    "# ▶️ 사용자 질의\n",
    "user_query = \"기침과 호흡 곤란이 있어요\"\n",
    "\n",
    "# ▶️ 질의 임베딩 생성\n",
    "query_embedding = model.encode([user_query]).tolist()\n",
    "\n",
    "# ▶️ Chroma 검색\n",
    "results = collection.query(\n",
    "    query_embeddings=query_embedding,\n",
    "    n_results=3\n",
    ")\n",
    "\n",
    "# ▶️ 관련 문서 텍스트 추출\n",
    "relevant_texts = results['documents'][0]\n",
    "context = \"\\n\".join(relevant_texts)\n",
    "\n",
    "gmodel = genai.GenerativeModel(\"gemini-pro\")\n",
    "response = gmodel.generate_content(f\"\"\"\n",
    "당신은 의학 상담 도우미입니다.\n",
    "다음은 참고할 증상 데이터입니다:\n",
    "\n",
    "{context}\n",
    "\n",
    "이 정보를 참고하여 사용자 질문에 답하세요:\n",
    "{user_query}\n",
    "\"\"\")\n",
    "\n",
    "print(\"🤖 Gemini 응답:\", response.text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
