{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "301eefa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/gemini-embedding-exp-03-07\n",
      "models/gemini-embedding-exp\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import chromadb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from chromadb import Documents, EmbeddingFunction, Embeddings\n",
    "\n",
    "from google import genai\n",
    "\n",
    "# PubMedQA 평가를 위한 라이브러리 추가\n",
    "from datasets import load_dataset\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "client = genai.Client(api_key='AIzaSyClUpDU2mSLVLMnQvcJBLTiKFGjeComQTA')\n",
    "\n",
    "\n",
    "for m in client.models.list():\n",
    "  if 'embedContent' in m.supported_actions:\n",
    "    print(m.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab9f7bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.genai import types\n",
    "\n",
    "class GeminiEmbeddingFunction(EmbeddingFunction):\n",
    "  def __call__(self, input: Documents) -> Embeddings:\n",
    "    EMBEDDING_MODEL_ID = \"models/text-embedding-004\"\n",
    "    title = \"Custom query\"\n",
    "    response = client.models.embed_content(\n",
    "        model=EMBEDDING_MODEL_ID,\n",
    "        contents=input,\n",
    "        config=types.EmbedContentConfig(\n",
    "          task_type=\"retrieval_document\",\n",
    "          title=title\n",
    "        )\n",
    "    )\n",
    "    # 모든 문서의 임베딩 벡터를 리스트로 반환\n",
    "    return [e.values for e in response.embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "322eb3e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dopar\\AppData\\Local\\Temp\\ipykernel_12228\\1664504859.py:27: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  embedding_function=GeminiEmbeddingFunction()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing collection 'pubmedqa_long_answers' to delete or error during deletion: Collection [pubmedqa_long_answers] does not exists\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dopar\\AppData\\Local\\Temp\\ipykernel_12228\\1664504859.py:56: DeprecationWarning: The class GeminiEmbeddingFunction does not implement __init__. This will be required in a future version.\n",
      "  embedding_function=GeminiEmbeddingFunction()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created and populated 'pubmedqa_long_answers' with 1000 documents.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "#chroma_client.delete_collection(\"my_collection\")\n",
    "def preprocess_metadata(metadata):\n",
    "    new_metadata = {}\n",
    "    for k, v in metadata.items():\n",
    "        if isinstance(v, list):\n",
    "            new_metadata[k] = \", \".join(map(str, v))  # 리스트를 문자열로 변환\n",
    "        else:\n",
    "            new_metadata[k] = v\n",
    "    return new_metadata\n",
    "def batch_add(collection, documents, metadatas, ids, batch_size=100):\n",
    "    for i in range(0, len(documents), batch_size):\n",
    "        batch_docs = documents[i:i+batch_size]\n",
    "        batch_metas = metadatas[i:i+batch_size]\n",
    "        batch_ids = ids[i:i+batch_size]\n",
    "        collection.add(\n",
    "            documents=batch_docs,\n",
    "            metadatas=batch_metas,\n",
    "            ids=batch_ids\n",
    "        )\n",
    "def create_chroma_db(json_data, name):\n",
    "    import chromadb\n",
    "\n",
    "    chroma_client = chromadb.Client()\n",
    "    db = chroma_client.create_collection(\n",
    "        name=name,\n",
    "        embedding_function=GeminiEmbeddingFunction()\n",
    "    )\n",
    "\n",
    "    # JSON 데이터에서 텍스트, 메타데이터, id 추출\n",
    "    documents = [item[\"text\"] for item in json_data]\n",
    "    metadatas = [preprocess_metadata(item[\"metadata\"]) for item in json_data]\n",
    "    ids = [str(i) for i in range(len(json_data))]\n",
    "    \n",
    "    batch_add(db, documents, metadatas, ids, batch_size=100)\n",
    "   \n",
    "    return db\n",
    "with open('disease_rag_with_metadata.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "db = create_chroma_db(data, \"my_collection\")\n",
    "\n",
    "# PubMedQA long_answer 임베딩을 위한 코드 추가\n",
    "def create_pubmedqa_long_answer_embeddings(pubmedqa_dataset, collection_name=\"pubmedqa_long_answers\"):\n",
    "    chroma_client = chromadb.Client()\n",
    "    \n",
    "    # Check if collection already exists and delete it to start fresh\n",
    "    try:\n",
    "        chroma_client.delete_collection(collection_name)\n",
    "        print(f\"Existing collection '{collection_name}' deleted.\")\n",
    "    except Exception as e:\n",
    "        print(f\"No existing collection '{collection_name}' to delete or error during deletion: {e}\")\n",
    "\n",
    "    db_long_answers = chroma_client.create_collection(\n",
    "        name=collection_name,\n",
    "        embedding_function=GeminiEmbeddingFunction()\n",
    "    )\n",
    "\n",
    "    documents = []\n",
    "    metadatas = []\n",
    "    ids = []\n",
    "\n",
    "    for i, entry in enumerate(pubmedqa_dataset['train']):\n",
    "        question = entry['question']\n",
    "        context = \" \".join(entry['context']['contexts'])\n",
    "        long_answer = entry['long_answer']\n",
    "        final_decision = entry['final_decision']\n",
    "\n",
    "        # Combine relevant information into a single document string for embedding\n",
    "        document_text = f\"Question: {question} Context: {context} Answer: {final_decision}\"\n",
    "        \n",
    "        documents.append(document_text)\n",
    "        metadatas.append({\n",
    "            \"question\": question,\n",
    "            \"final_decision\": final_decision,\n",
    "            \"context\": context\n",
    "        })\n",
    "        ids.append(str(i))\n",
    "    \n",
    "    batch_add(db_long_answers, documents, metadatas, ids, batch_size=100)\n",
    "    print(f\"Successfully created and populated '{collection_name}' with {len(documents)} documents.\")\n",
    "    return db_long_answers\n",
    "\n",
    "# PubMedQA 데이터셋 로드\n",
    "pubmedqa_dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "\n",
    "# long_answer 임베딩 및 Chroma DB 생성\n",
    "db_pubmedqa_long_answers = create_pubmedqa_long_answer_embeddings(pubmedqa_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9cb907b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  IDs                                          Documents  \\\n",
      "0   0  증상: Fever, Fatigue, Difficulty Breathing이(가) 있...   \n",
      "1   1  증상: Cough, Fatigue이(가) 있고, 나이: 25세, 성별: Female...   \n",
      "2   2  증상: Cough, Fatigue이(가) 있고, 나이: 25세, 성별: Female...   \n",
      "\n",
      "                                          Embeddings  \n",
      "0  [ 0.00477051  0.04616465 -0.06666899 -0.040444...  \n",
      "1  [-3.57129462e-02  7.49278739e-02 -6.56882823e-...  \n",
      "2  [-3.55055146e-02  4.89426367e-02 -6.10607862e-...  \n",
      "\n",
      "--- Sample PubMedQA Long Answer Embeddings ---\n",
      "  IDs                                          Documents  \\\n",
      "0   0  Question: Do mitochondria play a role in remod...   \n",
      "1   1  Question: Landolt C and snellen e acuity: diff...   \n",
      "2   2  Question: Syncope during bathing in infants, a...   \n",
      "\n",
      "                                           Metadatas  \\\n",
      "0  {'context': 'Programmed cell death (PCD) is th...   \n",
      "1  {'context': 'Assessment of visual acuity depen...   \n",
      "2  {'question': 'Syncope during bathing in infant...   \n",
      "\n",
      "                                          Embeddings  \n",
      "0  [-5.39701544e-02  4.39369865e-02 -6.92422614e-...  \n",
      "1  [ 2.89553385e-02  1.07113933e-02 -1.87315717e-...  \n",
      "2  [ 3.55442823e-03  2.20755651e-03 -6.24710210e-...  \n"
     ]
    }
   ],
   "source": [
    "sample_data = db.get(include=['documents', 'embeddings'])\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"IDs\": sample_data['ids'][:3],\n",
    "    \"Documents\": sample_data['documents'][:3],\n",
    "    \"Embeddings\": [str(emb)[:50] + \"...\" for emb in sample_data['embeddings'][:3]]  # Truncate embeddings\n",
    "})\n",
    "print(df)\n",
    "\n",
    "sample_long_answer_data = db_pubmedqa_long_answers.get(include=['documents', 'embeddings', 'metadatas'], limit=3)\n",
    "df_long_answers = pd.DataFrame({\n",
    "    \"IDs\": sample_long_answer_data['ids'],\n",
    "    \"Documents\": sample_long_answer_data['documents'],\n",
    "    \"Metadatas\": sample_long_answer_data['metadatas'],\n",
    "    \"Embeddings\": [str(emb)[:50] + \"...\" for emb in sample_long_answer_data['embeddings']]\n",
    "})\n",
    "print(\"\\n--- Sample PubMedQA Long Answer Embeddings ---\")\n",
    "print(df_long_answers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4861eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using general disease database (profile-specific query detected).\n",
      "\n",
      "--- Passages for General Query ---\n",
      "증상: Fever이(가) 있고, 나이: 31세, 성별: Female, 혈압: Normal, 콜레스테롤: Normal인 환자의 경우 Migraine일 수 있습니다. (나이: 31, 성별: Female, 혈압: Normal, 콜레스테롤: Normal)\n",
      "증상: Fever이(가) 있고, 나이: 25세, 성별: Female, 혈압: Normal, 콜레스테롤: Normal인 환자의 경우 Eczema일 수 있습니다. (나이: 25, 성별: Female, 혈압: Normal, 콜레스테롤: Normal)\n",
      "증상: Fever, Cough, Difficulty Breathing이(가) 있고, 나이: 30세, 성별: Female, 혈압: Normal, 콜레스테롤: Normal인 환자의 경우 Asthma일 수 있습니다. (나이: 30, 성별: Female, 혈압: Normal, 콜레스테롤: Normal)\n",
      "증상: Fever, Cough, Difficulty Breathing이(가) 있고, 나이: 30세, 성별: Female, 혈압: Normal, 콜레스테롤: Normal인 환자의 경우 Asthma일 수 있습니다. (나이: 30, 성별: Female, 혈압: Normal, 콜레스테롤: Normal)\n",
      "증상: Fever, Difficulty Breathing이(가) 있고, 나이: 42세, 성별: Female, 혈압: Normal, 콜레스테롤: Normal인 환자의 경우 Liver Disease일 수 있습니다. (나이: 42, 성별: Female, 혈압: Normal, 콜레스테롤: Normal)\n",
      "--------------------------------------------------\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "\n",
      "--- Passages for Medical QA Query ---\n",
      "Question: Can tailored interventions increase mammography use among HMO women? Context: Telephone counseling and tailored print communications have emerged as promising methods for promoting mammography screening. However, there has been little research testing, within the same randomized field trial, of the efficacy of these two methods compared to a high-quality usual care system for enhancing screening. This study addressed the question: Compared to usual care, is tailored telephone counseling more effective than tailored print materials for promoting mammography screening? Three-year randomized field trial. One thousand ninety-nine women aged 50 and older recruited from a health maintenance organization in North Carolina. Women were randomized to 1 of 3 groups: (1) usual care, (2) tailored print communications, and (3) tailored telephone counseling. Adherence to mammography screening based on self-reports obtained during 1995, 1996, and 1997. Compared to usual care alone, telephone counseling promoted a significantly higher proportion of women having mammograms on schedule (71% vs 61%) than did tailored print (67% vs 61%) but only after the first year of intervention (during 1996). Furthermore, compared to usual care, telephone counseling was more effective than tailored print materials at promoting being on schedule with screening during 1996 and 1997 among women who were off-schedule during the previous year. Answer: Question: Can tailored interventions increase mammography use among HMO women? Context: Telephone counseling and tailored print communications have emerged as promising methods for promoting mammography screening. However, there has been little research testing, within the same randomized field trial, of the efficacy of these two methods compared to a high-quality usual care system for enhancing screening. This study addressed the question: Compared to usual care, is tailored telephone counseling more effective than tailored print materials for promoting mammography screening? Three-year randomized field trial. One thousand ninety-nine women aged 50 and older recruited from a health maintenance organization in North Carolina. Women were randomized to 1 of 3 groups: (1) usual care, (2) tailored print communications, and (3) tailored telephone counseling. Adherence to mammography screening based on self-reports obtained during 1995, 1996, and 1997. Compared to usual care alone, telephone counseling promoted a significantly higher proportion of women having mammograms on schedule (71% vs 61%) than did tailored print (67% vs 61%) but only after the first year of intervention (during 1996). Furthermore, compared to usual care, telephone counseling was more effective than tailored print materials at promoting being on schedule with screening during 1996 and 1997 among women who were off-schedule during the previous year. Answer: yes\n",
      "Question: Prompting Primary Care Providers about Increased Patient Risk As a Result of Family History: Does It Work? Context: Electronic health records have the potential to facilitate family history use by primary care physicians (PCPs) to provide personalized care. The objective of this study was to determine whether automated, at-the-visit tailored prompts about family history risk change PCP behavior. Automated, tailored prompts highlighting familial risk for heart disease, stroke, diabetes, and breast, colorectal, or ovarian cancer were implemented during 2011 to 2012. Medical records of a cohort of community-based primary care patients, aged 35 to 65 years, who previously participated in our Family Healthware study and had a moderate or strong familial risk for any of the 6 diseases were subsequently reviewed. The main outcome measures were PCP response to the prompts, adding family history risk to problem summary lists, and patient screening status for each disease. The 492 eligible patients had 847 visits during the study period; 152 visits had no documentation of response to a family history prompt. Of the remaining 695 visits, physician responses were reviewed family history (n = 372, 53.5%), discussed family history (n = 159, 22.9%), not addressed (n = 155, 22.3%), and reviewed family history and ordered tests/referrals (n = 5, 0.7%). There was no significant change in problem summary list documentation of risk status or screening interventions for any of the 6 diseases. Answer: Question: Prompting Primary Care Providers about Increased Patient Risk As a Result of Family History: Does It Work? Context: Electronic health records have the potential to facilitate family history use by primary care physicians (PCPs) to provide personalized care. The objective of this study was to determine whether automated, at-the-visit tailored prompts about family history risk change PCP behavior. Automated, tailored prompts highlighting familial risk for heart disease, stroke, diabetes, and breast, colorectal, or ovarian cancer were implemented during 2011 to 2012. Medical records of a cohort of community-based primary care patients, aged 35 to 65 years, who previously participated in our Family Healthware study and had a moderate or strong familial risk for any of the 6 diseases were subsequently reviewed. The main outcome measures were PCP response to the prompts, adding family history risk to problem summary lists, and patient screening status for each disease. The 492 eligible patients had 847 visits during the study period; 152 visits had no documentation of response to a family history prompt. Of the remaining 695 visits, physician responses were reviewed family history (n = 372, 53.5%), discussed family history (n = 159, 22.9%), not addressed (n = 155, 22.3%), and reviewed family history and ordered tests/referrals (n = 5, 0.7%). There was no significant change in problem summary list documentation of risk status or screening interventions for any of the 6 diseases. Answer: no\n",
      "Question: Multidisciplinary breast cancer clinics. Do they work? Context: In an attempt to improve the care they provide for their patients with breast cancer, the authors' institution developed a multidisciplinary breast cancer clinic (MDBCC) to offer \"one-stop shopping\" consultation and support for newly diagnosed breast cancer patients. One hundred sixty-two patients, the control group for this study, were evaluated at Henry Ford Hospital during the year prior to the opening of the MDBCC. These patients, who were referred in the traditional sequential consultation manner, were compared with the first 177 patients seen during the first year of the clinic's operation. Retrospective chart reviews were conducted to assess treatment timeliness, and anonymous questionnaires were used to assess patient satisfaction. The authors found that the MDBCC increased patient satisfaction by encouraging involvement of patients' families and friends and by helping patients make treatment decisions (P<0.001). The time between diagnosis and the initiation of treatment was also significantly decreased (42.2 days vs. 29.6 days; P<0.0008). Answer: Question: Multidisciplinary breast cancer clinics. Do they work? Context: In an attempt to improve the care they provide for their patients with breast cancer, the authors' institution developed a multidisciplinary breast cancer clinic (MDBCC) to offer \"one-stop shopping\" consultation and support for newly diagnosed breast cancer patients. One hundred sixty-two patients, the control group for this study, were evaluated at Henry Ford Hospital during the year prior to the opening of the MDBCC. These patients, who were referred in the traditional sequential consultation manner, were compared with the first 177 patients seen during the first year of the clinic's operation. Retrospective chart reviews were conducted to assess treatment timeliness, and anonymous questionnaires were used to assess patient satisfaction. The authors found that the MDBCC increased patient satisfaction by encouraging involvement of patients' families and friends and by helping patients make treatment decisions (P<0.001). The time between diagnosis and the initiation of treatment was also significantly decreased (42.2 days vs. 29.6 days; P<0.0008). Answer: yes\n",
      "Question: Mammographic screening in Sami speaking municipalities and a control group. Are early outcome measures influenced by ethnicity? Context: Female citizens of Sami (the indigenous people of Norway) municipalities in northern Norway have a low risk of breast cancer. The objective of this study was to describe the attendance rate and outcome of the Norwegian Breast Cancer Screening Program (NBCSP) in the Sami-speaking municipalities and a control group. A retrospective registry-based study. The 8 municipalities included in the administration area of the Sami language law (Sami) were matched with a control group of 11 municipalities (non-Sami). Population data were accessed from Statistics Norway. Data regarding invitations and outcome in the NBCSP during the period 2001-2010 was derived from the Cancer Registry of Norway (CRN). The NBCSP targets women aged 50-69 years. Rates and percentages were compared using chi-square test with a p-value<0.05 as statistical significant. The attendance rate in the NBCSP was 78% in the Sami and 75% in the non-Sami population (p<0.01). The recall rates were 2.4 and 3.3% in the Sami and non-Sami population, respectively (p<0.01). The rate of invasive screen detected cancer was not significantly lower in the Sami group (p=0.14). The percentage of all breast cancers detected in the NBCSP among the Sami (67%) was lower compared with the non-Sami population (86%, p=0.06). Answer: Question: Mammographic screening in Sami speaking municipalities and a control group. Are early outcome measures influenced by ethnicity? Context: Female citizens of Sami (the indigenous people of Norway) municipalities in northern Norway have a low risk of breast cancer. The objective of this study was to describe the attendance rate and outcome of the Norwegian Breast Cancer Screening Program (NBCSP) in the Sami-speaking municipalities and a control group. A retrospective registry-based study. The 8 municipalities included in the administration area of the Sami language law (Sami) were matched with a control group of 11 municipalities (non-Sami). Population data were accessed from Statistics Norway. Data regarding invitations and outcome in the NBCSP during the period 2001-2010 was derived from the Cancer Registry of Norway (CRN). The NBCSP targets women aged 50-69 years. Rates and percentages were compared using chi-square test with a p-value<0.05 as statistical significant. The attendance rate in the NBCSP was 78% in the Sami and 75% in the non-Sami population (p<0.01). The recall rates were 2.4 and 3.3% in the Sami and non-Sami population, respectively (p<0.01). The rate of invasive screen detected cancer was not significantly lower in the Sami group (p=0.14). The percentage of all breast cancers detected in the NBCSP among the Sami (67%) was lower compared with the non-Sami population (86%, p=0.06). Answer: yes\n",
      "Question: Do follow-up recommendations for abnormal Papanicolaou smears influence patient adherence? Context: To compare adherence to follow-up recommendations for colposcopy or repeated Papanicolaou (Pap) smears for women with previously abnormal Pap smear results. Retrospective cohort study. Three northern California family planning clinics. All women with abnormal Pap smear results referred for initial colposcopy and a random sample of those referred for repeated Pap smear. Medical records were located and reviewed for 90 of 107 women referred for colposcopy and 153 of 225 women referred for repeated Pap smears. Routine clinic protocols for follow-up--telephone call, letter, or certified letter--were applied without regard to the type of abnormality seen on a Pap smear or recommended examination. Documented adherence to follow-up within 8 months of an abnormal result. Attempts to contact the patients for follow-up, adherence to follow-up recommendations, and patient characteristics were abstracted from medical records. The probability of adherence to follow-up vs the number of follow-up attempts was modeled with survival analysis. Cox proportional hazards models were used to examine multivariate relationships related to adherence. The rate of overall adherence to follow-up recommendations was 56.0% (136/243). Adherence to a second colposcopy was not significantly different from that to a repeated Pap smear (odds ratio, 1.40; 95% confidence interval, 0.80-2.46). The use of as many as 3 patient reminders substantially improved adherence to follow-up. Women without insurance and women attending 1 of the 3 clinics were less likely to adhere to any follow-up recommendation (hazard ratio for no insurance, 0.43 [95% confidence interval, 0.20-0.93], and for clinic, 0.35 [95% confidence interval, 0.15-0.73]). Answer: Question: Do follow-up recommendations for abnormal Papanicolaou smears influence patient adherence? Context: To compare adherence to follow-up recommendations for colposcopy or repeated Papanicolaou (Pap) smears for women with previously abnormal Pap smear results. Retrospective cohort study. Three northern California family planning clinics. All women with abnormal Pap smear results referred for initial colposcopy and a random sample of those referred for repeated Pap smear. Medical records were located and reviewed for 90 of 107 women referred for colposcopy and 153 of 225 women referred for repeated Pap smears. Routine clinic protocols for follow-up--telephone call, letter, or certified letter--were applied without regard to the type of abnormality seen on a Pap smear or recommended examination. Documented adherence to follow-up within 8 months of an abnormal result. Attempts to contact the patients for follow-up, adherence to follow-up recommendations, and patient characteristics were abstracted from medical records. The probability of adherence to follow-up vs the number of follow-up attempts was modeled with survival analysis. Cox proportional hazards models were used to examine multivariate relationships related to adherence. The rate of overall adherence to follow-up recommendations was 56.0% (136/243). Adherence to a second colposcopy was not significantly different from that to a repeated Pap smear (odds ratio, 1.40; 95% confidence interval, 0.80-2.46). The use of as many as 3 patient reminders substantially improved adherence to follow-up. Women without insurance and women attending 1 of the 3 clinics were less likely to adhere to any follow-up recommendation (hazard ratio for no insurance, 0.43 [95% confidence interval, 0.20-0.93], and for clinic, 0.35 [95% confidence interval, 0.15-0.73]). Answer: no\n",
      "--------------------------------------------------\n",
      "Using general disease database (default).\n",
      "\n",
      "--- Passages for Symptom Query ---\n",
      "증상: Fever, Fatigue, Difficulty Breathing이(가) 있고, 나이: 40세, 성별: Male, 혈압: Normal, 콜레스테롤: High인 환자의 경우 Asthma일 수 있습니다. (나이: 40, 성별: Male, 혈압: Normal, 콜레스테롤: High)\n",
      "증상: Fever, Fatigue, Difficulty Breathing이(가) 있고, 나이: 40세, 성별: Male, 혈압: Normal, 콜레스테롤: High인 환자의 경우 Asthma일 수 있습니다. (나이: 40, 성별: Male, 혈압: Normal, 콜레스테롤: High)\n",
      "증상: Fever, Fatigue, Difficulty Breathing이(가) 있고, 나이: 32세, 성별: Female, 혈압: High, 콜레스테롤: Normal인 환자의 경우 Pneumonia일 수 있습니다. (나이: 32, 성별: Female, 혈압: High, 콜레스테롤: Normal)\n",
      "증상: Fever, Cough, Fatigue, Difficulty Breathing이(가) 있고, 나이: 50세, 성별: Female, 혈압: Normal, 콜레스테롤: High인 환자의 경우 Asthma일 수 있습니다. (나이: 50, 성별: Female, 혈압: Normal, 콜레스테롤: High)\n",
      "증상: Fever, Cough, Fatigue, Difficulty Breathing이(가) 있고, 나이: 60세, 성별: Female, 혈압: High, 콜레스테롤: High인 환자의 경우 Asthma일 수 있습니다. (나이: 60, 성별: Female, 혈압: High, 콜레스테롤: High)\n"
     ]
    }
   ],
   "source": [
    "def get_relevant_passage(query, db, n_results=5):\n",
    "  results = db.query(query_texts=[query], n_results=n_results, include=['documents', 'metadatas'])\n",
    "  passages = []\n",
    "  for i in range(len(results['documents'][0])):\n",
    "      doc = results['documents'][0][i]\n",
    "      meta = results['metadatas'][0][i]\n",
    "      \n",
    "      passage_text = f\"{doc} (나이: {meta.get('age', '정보 없음')}, 성별: {meta.get('gender', '정보 없음')}, 혈압: {meta.get('blood_pressure', '정보 없음')}, 콜레스테롤: {meta.get('cholesterol', '정보 없음')})\"\n",
    "      passages.append(passage_text)\n",
    "  return passages\n",
    "# Perform embedding search\n",
    "passages = get_relevant_passage(\"Fever, Cough, Difficulty Breathing\", db, 5)\n",
    "\n",
    "\n",
    "def get_relevant_passage_intelligent(query, db_general, db_medical_qa, n_results=5):\n",
    "    \"\"\"\n",
    "    쿼리의 특성에 따라 적절한 ChromaDB 컬렉션에서 관련 구절을 가져옵니다.\n",
    "\n",
    "    Args:\n",
    "        query (str): 사용자 쿼리.\n",
    "        db_general (chromadb.api.models.Collection.Collection): 일반 질병 정보가 담긴 ChromaDB 컬렉션.\n",
    "        db_medical_qa (chromadb.api.models.Collection.Collection): PubMedQA long_answer가 담긴 ChromaDB 컬렉션.\n",
    "        n_results (int): 가져올 결과의 최대 개수.\n",
    "\n",
    "    Returns:\n",
    "        list: 관련 구절(문서 및 메타데이터 포함)의 리스트.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 쿼리 분석을 통해 어떤 DB를 사용할지 결정하는 로직 (예시)\n",
    "    # 실제 환경에서는 더 정교한 분류 모델이나 키워드 분석이 필요할 수 있습니다.\n",
    "    medical_keywords = [\"medication\", \"treatment\", \"diagnosis\", \"clinical\", \"study\", \"trial\", \"gene\", \"protein\", \"cell\", \"molecule\", \"pubmed\", \"article\", \"research\", \"efficacy\", \"mechanism\", \"pathway\", \"therapy\", \"syndrome\", \"disorder\"]\n",
    "    \n",
    "    # 쿼리에 의학 관련 키워드가 포함되어 있는지 확인\n",
    "    is_medical_query = any(keyword in query.lower() for keyword in medical_keywords)\n",
    "    \n",
    "    # 쿼리에 나이, 성별, 혈압, 콜레스테롤, 증상과 같은 개인 프로필 정보가 명시적으로 포함되어 있는지 확인\n",
    "    # 이는 'disease_rag_with_metadata.json' 데이터의 특성을 고려한 것입니다.\n",
    "    is_profile_specific_query = bool(re.search(r'(나이|성별|혈압|콜레스테롤|증상):', query))\n",
    "\n",
    "    selected_db = None\n",
    "    passages = []\n",
    "\n",
    "    if is_profile_specific_query:\n",
    "        # 개인 프로필 정보가 명시적으로 포함된 경우, 일반 질병 DB를 우선적으로 사용\n",
    "        print(\"Using general disease database (profile-specific query detected).\")\n",
    "        selected_db = db_general\n",
    "    elif is_medical_query:\n",
    "        # 의학 관련 키워드가 포함된 경우, PubMedQA DB를 사용\n",
    "        print(\"Using PubMedQA medical QA database (medical keywords detected).\")\n",
    "        selected_db = db_medical_qa\n",
    "    else:\n",
    "        # 둘 다 아닌 경우 (일반적인 증상 쿼리 등), 일반 질병 DB를 기본으로 사용\n",
    "        print(\"Using general disease database (default).\")\n",
    "        selected_db = db_general\n",
    "\n",
    "    if selected_db:\n",
    "        results = selected_db.query(query_texts=[query], n_results=n_results, include=['documents', 'metadatas'])\n",
    "        \n",
    "        for i in range(len(results['documents'][0])):\n",
    "            doc = results['documents'][0][i]\n",
    "            meta = results['metadatas'][0][i]\n",
    "            \n",
    "            # 선택된 DB에 따라 passage_text 형식을 조정\n",
    "            if selected_db == db_general:\n",
    "                passage_text = f\"{doc} (나이: {meta.get('age', '정보 없음')}, 성별: {meta.get('gender', '정보 없음')}, 혈압: {meta.get('blood_pressure', '정보 없음')}, 콜레스테롤: {meta.get('cholesterol', '정보 없음')})\"\n",
    "            elif selected_db == db_medical_qa:\n",
    "                # PubMedQA 데이터의 메타데이터 구조에 맞게 조정\n",
    "                passage_text = f\"Question: {meta.get('question', '정보 없음')} Context: {meta.get('context', '정보 없음')} Answer: {doc}\"\n",
    "            passages.append(passage_text)\n",
    "    else:\n",
    "        print(\"No suitable database found for the query.\")\n",
    "\n",
    "    return passages\n",
    "\n",
    "# 예시 사용:\n",
    "# 먼저, 위에서 정의한 db와 db_pubmedqa_long_answers가 생성되어 있어야 합니다.\n",
    "\n",
    "# 일반 질병 쿼리 예시\n",
    "query_general = \"나이: 30세, 성별: Female, 증상: 기침, 콧물, 인후통\"\n",
    "passages_general = get_relevant_passage_intelligent(query_general, db, db_pubmedqa_long_answers, 5)\n",
    "print(\"\\n--- Passages for General Query ---\")\n",
    "for p in passages_general:\n",
    "    print(p)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 의학 논문 쿼리 예시\n",
    "query_medical = \"Can tailored interventions increase mammography use among HMO women? A clinical study\"\n",
    "passages_medical = get_relevant_passage_intelligent(query_medical, db, db_pubmedqa_long_answers, 5)\n",
    "print(\"\\n--- Passages for Medical QA Query ---\")\n",
    "for p in passages_medical:\n",
    "    print(p)\n",
    "\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# 일반적인 증상 쿼리 (기본적으로 일반 질병 DB 사용)\n",
    "query_symptom = \"Fever, Fatigue, Difficulty Breathing\"\n",
    "passages_symptom = get_relevant_passage_intelligent(query_symptom, db, db_pubmedqa_long_answers, 5)\n",
    "print(\"\\n--- Passages for Symptom Query ---\")\n",
    "for p in passages_symptom:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5c7eede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using general disease database (profile-specific query detected).\n",
      "--- LLM 답변 ---\n",
      "안녕하세요! 심각한 공부하기 싫음 증상으로 힘드시군요. 학업에 대한 의욕이 저하되는 것은 누구에게나 있을 수 있는 일이지만, 그 원인을 파악하고 적절히 대처하는 것이 중요합니다.\n",
      "\n",
      "(임상 데이터에 따르면, 25세 남성 환자 중 정상 혈압과 콜레스테롤 수치를 가진 분들에게서 유사한 무기력감을 느끼는 사례가 보고되기도 했습니다.) 이러한 증상은 일시적인 스트레스나 번아웃 증후군과 관련이 있을 수 있습니다.\n",
      "\n",
      "번아웃 증후군은 장기간에 걸친 과도한 스트레스, 과로, 의욕 상실 등으로 인해 신체적, 정신적으로 지쳐버리는 상태를 말합니다. 단순히 '공부하기 싫은' 감정을 넘어, 무기력감, 짜증, 집중력 저하, 수면 장애 등 다양한 증상을 동반할 수 있습니다.\n",
      "\n",
      "이럴 때는 다음과 같은 생활 습관 개선을 통해 증상 완화에 도움을 줄 수 있습니다:\n",
      "\n",
      "*   **충분한 휴식:** 규칙적인 수면 습관을 유지하고, 주말이나 휴일을 이용해 충분한 휴식을 취하세요.\n",
      "*   **취미 활동:** 공부 외에 좋아하는 활동 (운동, 음악 감상, 영화 감상 등)을 통해 스트레스를 해소하고 활력을 되찾으세요.\n",
      "*   **규칙적인 생활:** 균형 잡힌 식단을 유지하고, 적절한 운동을 통해 신체 건강을 관리하세요.\n",
      "*   **학습 방법 변화:** 학습 계획을 재검토하고, 목표를 낮추거나 학습 방법을 바꿔보는 것도 도움이 될 수 있습니다.\n",
      "*   **긍정적인 마음 유지:** 주변 사람들과 소통하고, 긍정적인 생각을 하도록 노력하세요. 작은 성취에도 스스로를 칭찬하며 자신감을 높이는 것도 중요합니다.\n",
      "\n",
      "만약 '심각한 공부하기 싫음' 외에 불안, 우울, 무기력감, 식욕 부진, 수면 장애 등의 증상이 동반된다면, 단순한 스트레스가 아닌 다른 원인일 수도 있습니다. (드물게는, 만성 피로 증후군과 같은 질환과 관련이 있을 수도 있습니다.)\n",
      "\n",
      "더 궁금한 점이 있으시면 언제든지 다시 질문해주세요. 항상 건강하시길 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "def make_prompt(query, relevant_passages):\n",
    "  escaped = \" \".join([p.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\n\", \" \") for p in relevant_passages])\n",
    "  prompt = f\"\"\"\n",
    "  당신은 사용자의 증상과 개인 프로필 정보를 기반으로 질병을 설명하고, **일상생활에서 할 수 있는 구체적이고 실용적인 조언을 상세하게 제공하는** 의료 상담 도우미입니다. 당신의 답변은 정보가 풍부하고 친절하며, **극단적이거나 심각한 질병을 직접적으로 진단하거나 추천하는 뉘앙스를 피해야 합니다.** 답변은 최소 100단어 이상으로 작성해 주세요.\n",
    "\n",
    "  **단계별 지시사항:**\n",
    "  1. 사용자 질문을 이해하고 핵심 증상(예: 기침, 콧물)을 정확하고 상세하게 파악하세요.\n",
    "  2. 제공된 '관련 정보 (PASSAGE)'를 면밀히 검토하여 사용자 증상과 가장 밀접하게 일치하는 질병(들)을 식별하되, **데이터에 기반한 질병 연관성을 언급하되 불필요하게 심각성을 강조하지 마세요.**\n",
    "  3. **여러 질병이 검색될 경우, 가장 일반적이거나 흔한 질환(예: 감기, 알레르기)을 우선적으로 상세히 설명하고, 그 다음으로 관련된 다른 질병들도 간략하게 제시하세요.**\n",
    "  4. 나이, 성별, 혈압, 콜레스테롤 수치와 같은 환자 프로필 정보가 있다면, 이를 **답변의 서론 부분에 해당 질병이 특정 프로필의 환자에게서 관찰될 수 있는 '사례'로 자연스럽게 통합하여 설명의 깊이를 더하세요.** 질병 진단의 직접적인 근거로 오해되지 않도록 주의하세요.\n",
    "  5. 답변은 정보가 풍부하고 명확하며 친절하게 작성하며, 다음 **상세 권장 출력 형식**을 따르되, **세부적인 구문은 모델의 자연스러운 생성에 맡기세요.**\n",
    "\n",
    "  **상세 권장 출력 형식:**\n",
    "  안녕하세요! [사용자 질문에서 파악된 증상]이(가) 있으시군요. 불편하시겠지만, 몇 가지 가능한 원인과 생활 속 대처법을 함께 알아보겠습니다.\n",
    "\n",
    "  (선택적: 임상 데이터에 따르면, [관련 정보의 나이]세 [관련 정보의 성별] 환자 중 [관련 정보의 혈압] 혈압과 [관련 정보의 콜레스테롤] 콜레스테롤 수치를 가진 분들에게서 [해당 질병과 연결된 증상]이 관찰된 사례가 있습니다.) 이러한 증상들은 [관련 정보에서 찾은 가장 일반적이고 가능성 높은 질병]과 관련이 있을 수 있습니다.\n",
    "\n",
    "  [질병에 대한 간략한 추가 설명 (2-3문장)]. 이 질병의 일반적인 경과나 특징에 대해 간략히 설명해 주세요.\n",
    "\n",
    "  이럴 때는 다음과 같은 생활 습관 개선을 통해 증상 완화에 도움을 줄 수 있습니다:\n",
    "  - **충분한 휴식:** 몸이 회복하는 데 필요한 시간을 주세요.\n",
    "  - **수분 섭취:** 따뜻한 물, 차 등을 자주 마셔 목을 촉촉하게 유지하고 탈수를 예방하세요.\n",
    "  - **실내 환경 관리:** 적절한 실내 습도를 유지하고 환기를 자주 해주세요.\n",
    "  - **영양가 있는 음식 섭취:** 면역력 강화를 위해 비타민과 미네랄이 풍부한 음식을 드세요.\n",
    "  - [추가적인 일반적인 조언 1 (예: 스트레스 관리, 가벼운 운동 등)]\n",
    "  - [추가적인 일반적인 조언 2 (예: 마스크 착용, 손 씻기 등)]\n",
    "\n",
    "  만약 [사용자 질문에서 파악된 증상] 외에 다른 불편한 증상이 있거나, 현재 증상이 나아지지 않고 오히려 심해진다면 [다른 관련 질병]일 수도 있습니다. (이때, 극단적인 질병은 가급적 언급하지 않거나, \"드물게는 ~일 수도 있습니다\"와 같이 조심스러운 표현을 사용하세요.)\n",
    "\n",
    "  더 궁금한 점이 있으시면 언제든지 다시 질문해주세요. 항상 건강하시길 바랍니다.\n",
    "\n",
    "  아래는 참고할 수 있는 임상 데이터입니다:\n",
    "  - 사용자 질문 (QUESTION): \\\"{query}\\\"\n",
    "  - 관련 정보 (PASSAGE): \\\"{escaped}\\\"\n",
    "\n",
    "  **주의사항:**\n",
    "  - 병원 방문 및 전문적인 상담을 직접적으로 권유하는 문구는 최종 답변에 포함하지 마세요.\n",
    "  - PASSAGE에 영어 단어가 포함되어 있다면, 괄호 안에 한글 뜻을 함께 제공해 주세요.\n",
    "  - **제공된 정보 내에서 '기침'과 '습진'의 연관성이 있더라도, '기침'이라는 증상에 더 일반적이고 흔한 질병(예: Common Cold, Influenza)이 있다면 이를 우선적으로 고려하여 답변하세요.**\n",
    "  - **'말라리아'와 같이 심각한 질병은 사용자가 직접적으로 언급하지 않는 한, 일반적인 증상만으로는 추천하지 마세요.**\n",
    "\n",
    "  ANSWER:\n",
    "  \"\"\".format(query=query, relevant_passages=escaped)\n",
    "  return prompt\n",
    "# 예시 사용\n",
    "query = \"나이: 25세, 성별: Male, 혈압: Normal, 콜레스테롤: Normal, 증상 : 심각한 공부하기 싫음\"\n",
    "passages = get_relevant_passage_intelligent(query_general, db, db_pubmedqa_long_answers, 5)\n",
    "prompt = make_prompt(query, passages)\n",
    "\n",
    "#print(prompt)\n",
    "\n",
    "MODEL_ID = \"gemini-2.0-flash\"\n",
    "answer = client.models.generate_content(\n",
    "    model = MODEL_ID,\n",
    "    contents = prompt\n",
    ")\n",
    "# 변경 시작: 'ANSWER:' 이후의 텍스트만 추출하고 앞뒤 공백 제거\n",
    "final_answer = answer.text.split(\"ANSWER:\")\n",
    "if len(final_answer) > 1:\n",
    "    final_answer = final_answer[1].strip()\n",
    "else:\n",
    "    final_answer = answer.text.strip()\n",
    "   \n",
    "# 6. 결과 출력\n",
    "print(\"--- LLM 답변 ---\")\n",
    "print(final_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d5df3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PubMedQA dataset...\n",
      "PubMedQA test set loaded with 1000 examples.\n",
      "\n",
      "Evaluating on 50 PubMedQA samples for Accuracy...\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 15)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 16)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 17)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 18)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 20)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 38)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 39)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 40)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 41)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 42)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 43)\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 44)\n",
      "\n",
      "--- PubMedQA 평가 결과 (정확도) ---\n",
      "평가 샘플 수: 38\n",
      "정확도: 0.1053\n",
      "\n",
      "--- PubMedQA 평가 예시 (정확도) ---\n",
      "질문: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\n",
      "정답 (Ground Truth Decision): yes\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n",
      "질문: Landolt C and snellen e acuity: differences in strabismus amblyopia?\n",
      "정답 (Ground Truth Decision): no\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n",
      "질문: Syncope during bathing in infants, a pediatric form of water-induced urticaria?\n",
      "정답 (Ground Truth Decision): yes\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n",
      "질문: Are the long-term results of the transanal pull-through equal to those of the transabdominal pull-through?\n",
      "정답 (Ground Truth Decision): no\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n",
      "질문: Can tailored interventions increase mammography use among HMO women?\n",
      "정답 (Ground Truth Decision): yes\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 기존 make_prompt_for_pubmedqa 함수는 긴 답변을 유도하므로, 짧은 답변을 위한 새로운 프롬프트 함수가 필요합니다.\n",
    "def make_short_answer_prompt_for_pubmedqa(query, relevant_passages):\n",
    "    escaped_passages = \" \".join([p.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\\\n\", \" \") for p in relevant_passages])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    당신은 의학적 질문에 대해 'Yes', 'No', 'Maybe'로만 답변하는 의료 AI 도우미입니다.\n",
    "    제공된 '참고 정보'를 바탕으로 '의학적 질문'에 대한 답변을 생성하세요.\n",
    "\n",
    "    **다음 지침을 엄격히 따르세요:**\n",
    "    1.  **'Yes'로 답변하는 경우:** '참고 정보'에 '의학적 질문'에 대한 직접적이고 명확한 긍정적 증거가 있을 경우에 'Yes'라고 답변하세요.\n",
    "    2.  **'No'로 답변하는 경우:** '참고 정보'에 '의학적 질문'에 대한 직접적이고 명확한 부정적 증거가 있거나, 질문의 내용이 '참고 정보'와 명백히 상반될 경우에 'No'라고 답변하세요.\n",
    "    3.  **'No'로 답변하는 경우:** 만약 확실한 근거가 없다면 'Maybe'라고 답변하세요. 그러나 긍정적/부정적 근거가 명확하다면 반드시 'Yes' 또는 'No'로 답변해야 합니다\n",
    "    다른 어떠한 설명도 추가하지 말고, 오직 하나의 단어('Yes', 'No', 'Maybe')로만 답변해야 합니다.\n",
    "\n",
    "    **의학적 질문 (QUESTION):** \\\"{query}\\\"\n",
    "    **참고 정보 (PASSAGE):** \\\"{escaped_passages}\\\"\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 평가 함수 변경: ROUGE 대신 정확도 측정\n",
    "def evaluate_accuracy_pubmedqa(llm_response: str, ground_truth_decision: str) -> bool:\n",
    "    # 모델의 응답을 정규화 (대소문자 무시, 공백 제거 등)\n",
    "    normalized_llm_response = llm_response.strip().lower()\n",
    "    normalized_ground_truth = ground_truth_decision.strip().lower()\n",
    "\n",
    "    return normalized_llm_response == normalized_ground_truth\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 메인 평가 실행 부분 수정\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nLoading PubMedQA dataset...\")\n",
    "pubmedqa_dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "pubmedqa_test_data = pubmedqa_dataset['train']\n",
    "\n",
    "print(f\"PubMedQA test set loaded with {len(pubmedqa_test_data)} examples.\")\n",
    "\n",
    "all_accuracies = []\n",
    "example_evaluations = []\n",
    "\n",
    "num_samples_to_evaluate = 50\n",
    "\n",
    "print(f\"\\nEvaluating on {num_samples_to_evaluate} PubMedQA samples for Accuracy...\")\n",
    "\n",
    "for i, entry in enumerate(pubmedqa_test_data):\n",
    "    if i >= num_samples_to_evaluate:\n",
    "        break\n",
    "\n",
    "    question = entry['question']\n",
    "    ground_truth_decision = entry['final_decision'] # 'long_answer' 대신 'final_decision' 사용\n",
    "\n",
    "    relevant_passages = get_relevant_passage(question, db, 5)\n",
    "\n",
    "    # 새로운 짧은 답변 프롬프트 사용\n",
    "    prompt = make_short_answer_prompt_for_pubmedqa(question, relevant_passages)\n",
    "\n",
    "    try:\n",
    "        MODEL_ID = \"gemini-2.0-flash\"\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=prompt\n",
    "        )\n",
    "\n",
    "        llm_response = response.text.split(\"ANSWER:\")\n",
    "        if len(llm_response) > 1:\n",
    "            llm_response = llm_response[1].strip()\n",
    "        else:\n",
    "            llm_response = response.text.strip()\n",
    "\n",
    "        # 정확도 평가\n",
    "        is_correct = evaluate_accuracy_pubmedqa(llm_response, ground_truth_decision)\n",
    "        all_accuracies.append(is_correct)\n",
    "\n",
    "        if i < 5:\n",
    "            example_evaluations.append({\n",
    "                'question': question,\n",
    "                'ground_truth_decision': ground_truth_decision,\n",
    "                'llm_response': llm_response,\n",
    "                'is_correct': is_correct\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"429 RESOURCE_EXHAUSTED\" in str(e):\n",
    "            retry_after_seconds = 5\n",
    "            print(f\"Quota exceeded. Retrying after {retry_after_seconds} seconds... (Sample {i})\")\n",
    "            time.sleep(retry_after_seconds)\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Error processing sample {i}: {e}\")\n",
    "            all_accuracies.append(False) # 오류 발생 시 오답으로 처리\n",
    "\n",
    "# 평균 정확도 계산\n",
    "if all_accuracies:\n",
    "    total_correct = sum(all_accuracies)\n",
    "    accuracy = total_correct / len(all_accuracies)\n",
    "    print(\"\\n--- PubMedQA 평가 결과 (정확도) ---\")\n",
    "    print(f\"평가 샘플 수: {len(all_accuracies)}\")\n",
    "    print(f\"정확도: {accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo evaluation results to display.\")\n",
    "\n",
    "# 예시 평가 결과 출력\n",
    "print(\"\\n--- PubMedQA 평가 예시 (정확도) ---\")\n",
    "for ex in example_evaluations:\n",
    "    print(f\"질문: {ex['question']}\")\n",
    "    print(f\"정답 (Ground Truth Decision): {ex['ground_truth_decision']}\")\n",
    "    print(f\"LLM 응답: {ex['llm_response']}\")\n",
    "    print(f\"정확도 일치 여부: {ex['is_correct']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5206714d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading PubMedQA dataset...\n",
      "PubMedQA test set loaded with 1000 examples.\n",
      "\n",
      "Evaluating on 50 PubMedQA samples for Accuracy...\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using general disease database (default).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using general disease database (default).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 10)\n",
      "Using general disease database (default).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 11)\n",
      "Using general disease database (default).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 12)\n",
      "Using general disease database (default).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 13)\n",
      "Using general disease database (default).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 14)\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 15)\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using general disease database (default).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using general disease database (default).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 31)\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using general disease database (default).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 33)\n",
      "Using general disease database (default).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 34)\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 35)\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using general disease database (default).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 37)\n",
      "Using general disease database (default).\n",
      "Quota exceeded. Retrying after 5 seconds... (Sample 38)\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using general disease database (default).\n",
      "Using PubMedQA medical QA database (medical keywords detected).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "Using general disease database (default).\n",
      "\n",
      "--- PubMedQA 평가 결과 (정확도) ---\n",
      "평가 샘플 수: 38\n",
      "정확도: 0.2368\n",
      "\n",
      "--- PubMedQA 평가 예시 (정확도) ---\n",
      "질문: Do mitochondria play a role in remodelling lace plant leaves during programmed cell death?\n",
      "정답 (Ground Truth Decision): yes\n",
      "LLM 응답: Yes\n",
      "정확도 일치 여부: True\n",
      "--------------------------------------------------\n",
      "질문: Landolt C and snellen e acuity: differences in strabismus amblyopia?\n",
      "정답 (Ground Truth Decision): no\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n",
      "질문: Syncope during bathing in infants, a pediatric form of water-induced urticaria?\n",
      "정답 (Ground Truth Decision): yes\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n",
      "질문: Are the long-term results of the transanal pull-through equal to those of the transabdominal pull-through?\n",
      "정답 (Ground Truth Decision): no\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n",
      "질문: Can tailored interventions increase mammography use among HMO women?\n",
      "정답 (Ground Truth Decision): yes\n",
      "LLM 응답: Maybe\n",
      "정확도 일치 여부: False\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# 기존 make_prompt_for_pubmedqa 함수는 긴 답변을 유도하므로, 짧은 답변을 위한 새로운 프롬프트 함수가 필요합니다.\n",
    "def make_short_answer_prompt_for_pubmedqa(query, relevant_passages):\n",
    "    escaped_passages = \" \".join([p.replace(\"'\", \"\").replace('\"', \"\").replace(\"\\\\n\", \" \") for p in relevant_passages])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    당신은 의학적 질문에 대해 'Yes', 'No', 'Maybe'로만 답변하는 의료 AI 도우미입니다.\n",
    "    제공된 '참고 정보'를 바탕으로 '의학적 질문'에 대한 답변을 생성하세요.\n",
    "\n",
    "    **다음 지침을 엄격히 따르세요:**\n",
    "    1.  **'Yes'로 답변하는 경우:** '참고 정보'에 '의학적 질문'에 대한 직접적이고 명확한 긍정적 증거가 있을 경우에 'Yes'라고 답변하세요.\n",
    "    2.  **'No'로 답변하는 경우:** '참고 정보'에 '의학적 질문'에 대한 직접적이고 명확한 부정적 증거가 있거나, 질문의 내용이 '참고 정보'와 명백히 상반될 경우에 'No'라고 답변하세요.\n",
    "    3.  **'No'로 답변하는 경우:** 만약 확실한 근거가 없다면 'Maybe'라고 답변하세요. 그러나 긍정적/부정적 근거가 명확하다면 반드시 'Yes' 또는 'No'로 답변해야 합니다\n",
    "    다른 어떠한 설명도 추가하지 말고, 오직 하나의 단어('Yes', 'No', 'Maybe')로만 답변해야 합니다.\n",
    "\n",
    "    **의학적 질문 (QUESTION):** \\\"{query}\\\"\n",
    "    **참고 정보 (PASSAGE):** \\\"{escaped_passages}\\\"\n",
    "\n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    return prompt\n",
    "\n",
    "# 평가 함수 변경: ROUGE 대신 정확도 측정\n",
    "def evaluate_accuracy_pubmedqa(llm_response: str, ground_truth_decision: str) -> bool:\n",
    "    # 모델의 응답을 정규화 (대소문자 무시, 공백 제거 등)\n",
    "    normalized_llm_response = llm_response.strip().lower()\n",
    "    normalized_ground_truth = ground_truth_decision.strip().lower()\n",
    "\n",
    "    return normalized_llm_response == normalized_ground_truth\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "# 메인 평가 실행 부분 수정\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "print(\"\\nLoading PubMedQA dataset...\")\n",
    "pubmedqa_dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "pubmedqa_test_data = pubmedqa_dataset['train']\n",
    "\n",
    "print(f\"PubMedQA test set loaded with {len(pubmedqa_test_data)} examples.\")\n",
    "\n",
    "all_accuracies = []\n",
    "example_evaluations = []\n",
    "\n",
    "num_samples_to_evaluate = 50\n",
    "\n",
    "print(f\"\\nEvaluating on {num_samples_to_evaluate} PubMedQA samples for Accuracy...\")\n",
    "\n",
    "for i, entry in enumerate(pubmedqa_test_data):\n",
    "    if i >= num_samples_to_evaluate:\n",
    "        break\n",
    "\n",
    "    question = entry['question']\n",
    "    ground_truth_decision = entry['final_decision'] # 'long_answer' 대신 'final_decision' 사용\n",
    "\n",
    "    relevant_passages = get_relevant_passage_intelligent(question, db, db_pubmedqa_long_answers, 5)\n",
    "\n",
    "    # 새로운 짧은 답변 프롬프트 사용\n",
    "    prompt = make_short_answer_prompt_for_pubmedqa(question, relevant_passages)\n",
    "\n",
    "    try:\n",
    "        MODEL_ID = \"gemini-2.0-flash\"\n",
    "        response = client.models.generate_content(\n",
    "            model=MODEL_ID,\n",
    "            contents=prompt\n",
    "        )\n",
    "\n",
    "        llm_response = response.text.split(\"ANSWER:\")\n",
    "        if len(llm_response) > 1:\n",
    "            llm_response = llm_response[1].strip()\n",
    "        else:\n",
    "            llm_response = response.text.strip()\n",
    "\n",
    "        # 정확도 평가\n",
    "        is_correct = evaluate_accuracy_pubmedqa(llm_response, ground_truth_decision)\n",
    "        all_accuracies.append(is_correct)\n",
    "\n",
    "        if i < 5:\n",
    "            example_evaluations.append({\n",
    "                'question': question,\n",
    "                'ground_truth_decision': ground_truth_decision,\n",
    "                'llm_response': llm_response,\n",
    "                'is_correct': is_correct\n",
    "            })\n",
    "\n",
    "    except Exception as e:\n",
    "        if \"429 RESOURCE_EXHAUSTED\" in str(e):\n",
    "            retry_after_seconds = 5\n",
    "            print(f\"Quota exceeded. Retrying after {retry_after_seconds} seconds... (Sample {i})\")\n",
    "            time.sleep(retry_after_seconds)\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"Error processing sample {i}: {e}\")\n",
    "            all_accuracies.append(False) # 오류 발생 시 오답으로 처리\n",
    "\n",
    "# 평균 정확도 계산\n",
    "if all_accuracies:\n",
    "    total_correct = sum(all_accuracies)\n",
    "    accuracy = total_correct / len(all_accuracies)\n",
    "    print(\"\\n--- PubMedQA 평가 결과 (정확도) ---\")\n",
    "    print(f\"평가 샘플 수: {len(all_accuracies)}\")\n",
    "    print(f\"정확도: {accuracy:.4f}\")\n",
    "else:\n",
    "    print(\"\\nNo evaluation results to display.\")\n",
    "\n",
    "# 예시 평가 결과 출력\n",
    "print(\"\\n--- PubMedQA 평가 예시 (정확도) ---\")\n",
    "for ex in example_evaluations:\n",
    "    print(f\"질문: {ex['question']}\")\n",
    "    print(f\"정답 (Ground Truth Decision): {ex['ground_truth_decision']}\")\n",
    "    print(f\"LLM 응답: {ex['llm_response']}\")\n",
    "    print(f\"정확도 일치 여부: {ex['is_correct']}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyhome",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
